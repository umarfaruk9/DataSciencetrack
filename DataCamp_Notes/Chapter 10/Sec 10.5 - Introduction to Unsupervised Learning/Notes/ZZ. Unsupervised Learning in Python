
Unsupervised Learning in Python

Course Description
Say you have a collection of customers with a variety of characteristics such as age, location, and financial history, and you wish to discover patterns and sort them into clusters. Or perhaps you have a set of texts, such as wikipedia pages, and you wish to segment them into categories based on their content. This is the world of unsupervised learning, called as such because you are not guiding, or supervising, the pattern discovery by some prediction task, but instead uncovering hidden structure from unlabeled data. Unsupervised learning encompasses a variety of techniques in machine learning, from clustering to dimension reduction to matrix factorization. In this course, you'll learn the fundamentals of unsupervised learning and implement the essential algorithms using scikit-learn and scipy. You will learn how to cluster, transform, visualize, and extract insights from unlabeled datasets, and end the course by building a recommender system to recommend popular musical artists.


___________________________________________________________________________________________________________

1
Clustering for dataset exploration
FREE
0%
Learn how to discover the underlying groups (or "clusters") in a dataset. By the end of this chapter, you'll be clustering companies using their stock market prices, and distinguishing different species by clustering their measurements.



Unsupervised learning
● Unsupervised learning finds pa!erns in data
● E.g. clustering customers by their purchases
● Compressing the data using purchase pa!erns (dimension
reduction)



Supervised vs unsupervised learning
● Supervised learning finds pa!erns for a prediction task
● E.g. classify tumors as benign or cancerous (labels)
● Unsupervised learning finds pa!erns in data
● ... but without a specific prediction task in mind



Iris dataset
● Measurements of many iris plants
● 3 species of iris: setosa, versicolor, virginica
● Petal length, petal width, sepal length, sepal width (the
features of the dataset)


Arrays, features & samples
● 2D NumPy array
● Columns are measurements (the features)
● Rows represent iris plants (the samples)



Iris data is 4-dimensional
● Iris samples are points in 4 dimensional space
● Dimension = number of features
● Dimension too high to visualize!
● ... but unsupervised learning gives insight



k-means clustering
● Finds clusters of samples
● Number of clusters must be specified
● Implemented in sklearn ("scikit-learn")



k-means clustering with scikit-learn
In [1]: print(samples)
[[ 5. 3.3 1.4 0.2]
 [ 5. 3.5 1.3 0.3]
 [ 4.9 2.4 3.3 1. ]
 [ 6.3 2.8 5.1 1.5]
 ...
 [ 7.2 3.2 6. 1.8]]
In [2]: from sklearn.cluster import KMeans
In [3]: model = KMeans(n_clusters=3)
In [4]: model.fit(samples)
Out[4]: KMeans(algorithm='auto', ...)
In [5]: labels = model.predict(samples)
In [6]: print(labels)
[0 0 1 1 0 1 2 1 0 1 ...]



Cluster labels for new samples
● New samples can be assigned to existing clusters
● k-means remembers the mean of each cluster (the "centroids")
● Finds the nearest centroid to each new sample



Cluster labels for new samples
In [7]: print(new_samples)
[[ 5.7 4.4 1.5 0.4]
 [ 6.5 3. 5.5 1.8]
 [ 5.8 2.7 5.1 1.9]]
In [8]: new_labels = model.predict(new_samples)
In [9]: print(new_labels)
[0 2 1]


Sca!er plots
● Sca!er plot of sepal length vs petal length
● Each point represents an iris sample
● Color points by cluster labels
● PyPlot (matplotlib.pyplot)


Sca!er plots
In [1]: import matplotlib.pyplot as plt
In [2]: xs = samples[:,0]
In [3]: ys = samples[:,2]
In [4]: plt.scatter(xs, ys, c=labels)
In [5]: plt.show()


_______________________________________________________________________________________________________________

Clustering 2D points
From the scatter plot of the previous exercise, you saw that the points seem to separate into 3 clusters. You'll now create a KMeans model to find 3 clusters, and fit it to the data points from the previous exercise. After the model has been fit, you'll obtain the cluster labels for some new points using the .predict() method.

You are given the array points from the previous exercise, and also an array new_points.

Instructions
100 XP
Import KMeans from sklearn.cluster.
Using KMeans(), create a KMeans instance called model to find 3 clusters. To specify the number of clusters, use the n_clusters keyword argument.
Use the .fit() method of model to fit the model to the array of points points.
Use the .predict() method of model to predict the cluster labels of new_points, assigning the result to labels.
Hit 'Submit Answer' to see the cluster labels of new_points.



# Import KMeans
from sklearn.cluster import KMeans

# Create a KMeans instance with 3 clusters: model
model = KMeans(n_clusters=3)

# Fit model to points
model.fit(points)

# Determine the cluster labels of new_points: labels
labels = model.predict(new_points)

# Print cluster labels of new_points
print(labels)


Great work! You've successfully performed k-Means clustering and predicted the labels of new points. But it is not easy to inspect the clustering by just looking at the printed labels. A visualization would be far more useful. In the next exercise, you'll inspect your clustering with a scatter plot!

________________________________________________________________________________________________________________


Inspect your clustering
Let's now inspect the clustering you performed in the previous exercise!

A solution to the previous exercise has already run, so new_points is an array of points and labels is the array of their cluster labels.

Instructions
100 XP
Import matplotlib.pyplot as plt.
Assign column 0 of new_points to xs, and column 1 of new_points to ys.
Make a scatter plot of xs and ys, specifying the c=labels keyword arguments to color the points by their cluster label. Also specify alpha=0.5.
Compute the coordinates of the centroids using the .cluster_centers_ attribute of model.
Assign column 0 of centroids to centroids_x, and column 1 of centroids to centroids_y.
Make a scatter plot of centroids_x and centroids_y, using 'D' (a diamond) as a marker by specifying the marker parameter. Set the size of the markers to be 50 using s=50.


# Import pyplot
import matplotlib.pyplot as plt

# Assign the columns of new_points: xs and ys
xs = new_points[:,0]
ys = new_points[:,1]

# Make a scatter plot of xs and ys, using labels to define the colors
plt.scatter(xs, ys, c=labels, alpha=0.5)

# Assign the cluster centers: centroids
centroids = model.cluster_centers_

# Assign the columns of centroids: centroids_x, centroids_y
centroids_x = centroids[:,0]
centroids_y = centroids[:,1]

# Make a scatter plot of centroids_x and centroids_y
plt.scatter(centroids_x, centroids_y, marker='D', s=50)
plt.show()


Fantastic! The clustering looks great! But how can you be sure that 3 clusters is the correct choice? In other words, how can you evaluate the quality of a clustering? Tune into the next video in which Ben will explain how to evaluate a clustering!



________________________________________________________________________________________________________________

Evaluating a
clustering


Evaluating a clustering
● Can check correspondence with e.g. iris species
● … but what if there are no species to check against?
● Measure quality of a clustering
● Informs choice of how many clusters to look for


Iris: clusters vs species
● k-means found 3 clusters amongst the iris samples
● Do the clusters correspond to the species?


Cross tabulation with pandas
● Clusters vs species is a "cross-tabulation"
● Use the pandas library
● Given the species of each sample as a list species
In [1]: print(species)
['setosa', 'setosa', 'versicolor', 'virginica', ... ] 


Aligning labels and species
In [2]: import pandas as pd
In [3]: df = pd.DataFrame({'labels': labels, 'species': species})
In [4]: print(df)
 labels species
0 1 setosa
1 1 setosa
2 2 versicolor
3 2 virginica
4 1 setosa
...


Crosstab of labels and species
In [5]: ct = pd.crosstab(df['labels'], df['species'])
In [6]: print(ct)
species setosa versicolor virginica
labels
0 0 2 36
1 50 0 0
2 0 48 14
How to evaluate a clustering, if there were no species
information?



Measuring clustering quality
● Using only samples and their cluster labels
● A good clustering has tight clusters
● ... and samples in each cluster bunched together



Inertia measures clustering quality
● Measures how spread out the clusters are (lower is be!er)
● Distance from each sample to centroid of its cluster
● A"er fit(), available as a!ribute inertia_
● k-means a!empts to minimize the inertia when choosing clusters

In [1]: from sklearn.cluster import KMeans
In [2]: model = KMeans(n_clusters=3)
In [3]: model.fit(samples)
In [4]: print((model.inertia_)




The number of clusters
● Clusterings of the iris dataset with different numbers of clusters
● More clusters means lower inertia
● What is the best number of clusters?


How many clusters to choose?
● A good clustering has tight clusters (so low inertia)
● ... but not too many clusters!
● Choose an "elbow" in the inertia plot
● Where inertia begins to decrease more slowly
● E.g. for iris dataset, 3 is a good choice

_______________________________________________________________________________________________________________

How many clusters of grain?
In the video, you learned how to choose a good number of clusters for a dataset using the k-means inertia graph. You are given an array samples containing the measurements (such as area, perimeter, length, and several others) of samples of grain. What's a good number of clusters in this case?

KMeans and PyPlot (plt) have already been imported for you.

This dataset was sourced from the UCI Machine Learning Repository.

Instructions
100 XP
For each of the given values of k, perform the following steps:
Create a KMeans instance called model with k clusters.
Fit the model to the grain data samples.
Append the value of the inertia_ attribute of model to the list inertias.
The code to plot ks vs inertias has been written for you, so hit 'Submit Answer' to see the plot!

ks = range(1, 6)
inertias = []

for k in ks:
    # Create a KMeans instance with k clusters: model
    model = KMeans(n_clusters=k)
    
    # Fit model to samples
    model.fit(samples)
    
    # Append the inertia to the list of inertias
    inertias.append(model.inertia_)
    
# Plot ks vs inertias
plt.plot(ks, inertias, '-o')
plt.xlabel('number of clusters, k')
plt.ylabel('inertia')
plt.xticks(ks)
plt.show()

Excellent job! The inertia decreases very slowly from 3 clusters to 4, so it looks like 3 clusters would be a good choice for this data.

_______________________________________________________________________________________________________________

Evaluating the grain clustering
In the previous exercise, you observed from the inertia plot that 3 is a good number of clusters for the grain data. In fact, the grain samples come from a mix of 3 different grain varieties: "Kama", "Rosa" and "Canadian". In this exercise, cluster the grain samples into three clusters, and compare the clusters to the grain varieties using a cross-tabulation.

You have the array samples of grain samples, and a list varieties giving the grain variety for each sample. Pandas (pd) and KMeans have already been imported for you.

Instructions
100 XP
Create a KMeans model called model with 3 clusters.
Use the .fit_predict() method of model to fit it to samples and derive the cluster labels. Using .fit_predict() is the same as using .fit() followed by .predict().
Create a DataFrame df with two columns named 'labels' and 'varieties', using labels and varieties, respectively, for the column values. This has been done for you.
Use the pd.crosstab() function on df['labels'] and df['varieties'] to count the number of times each grain variety coincides with each cluster label. Assign the result to ct.
Hit 'Submit Answer' to see the cross-tabulation!


# Create a KMeans model with 3 clusters: model
model = KMeans(n_clusters = 3)

# Use fit_predict to fit model and obtain cluster labels: labels
labels = model.fit_predict(samples)

# Create a DataFrame with labels and varieties as columns: df
df = pd.DataFrame({'labels': labels, 'varieties': varieties})

# Create crosstab: ct
ct = pd.crosstab(df['labels'], df['varieties'])

# Display ct
print(ct)



<script.py> output:
    varieties  Canadian wheat  Kama wheat  Rosa wheat
    labels                                           
    0                      68           9           0
    1                       0           1          60
    2                       2          60          10



Great work! The cross-tabulation shows that the 3 varieties of grain separate really well into 3 clusters. But depending on the type of data you are working with, the clustering may not always be this good. Is there anything you can do in such situations to improve your clustering? You'll find out in the next video!


_______________________________________________________________________________________________________________

Transforming features
for be!er clusterings




Piedmont wines dataset
● 178 samples from 3 distinct varieties of red wine: Barolo,
Grignolino and Barbera
● Features measure chemical composition e.g. alcohol content
● … also visual properties like “color intensity”




Clustering the wines
In [1]: from sklearn.cluster import KMeans
In [2]: model = KMeans(n_clusters=3)
In [3]: labels = model.fit_predict(samples)



Clusters vs. varieties
In [4]: df = pd.DataFrame({'labels': labels,
 ...: 'varieties': varieties})
In [5]: ct = pd.crosstab(df['labels'], df['varieties'])
In [6]: print(ct)
varieties Barbera Barolo Grignolino
labels
0 29 13 20
1 0 46 1
2 19 0 50



Feature variances
feature variance
alcohol 0.65
malic_acid 1.24
...
od280 0.50
proline 99166.71
● The wine features have very different variances!
● Variance of a feature measures spread of its values



StandardScaler
● In kmeans: feature variance = feature influence
● StandardScaler transforms each feature to have mean 0 and variance 1
● Features are said to be "standardized"



sklearn StandardScaler
In [1]: from sklearn.preprocessing import StandardScaler
In [2]: scaler = StandardScaler()
In [3]: scaler.fit(samples)
Out[3]: StandardScaler(copy=True, with_mean=True, with_std=True)
In [4]: samples_scaled = scaler.transform(samples)



Similar methods
● StandardScaler and KMeans have similar methods
● Use fit() / transform() with StandardScaler
● Use fit() / predict() with KMeans




StandardScaler, then KMeans
● Need to perform two steps: StandardScaler, then
KMeans
● Use sklearn pipeline to combine multiple steps
● Data flows from one step into the next



Pipelines combine multiple steps
In [1]: from sklearn.preprocessing import StandardScaler
In [2]: from sklearn.cluster import KMeans
In [3]: scaler = StandardScaler()
In [4]: kmeans = KMeans(n_clusters=3)
In [5]: from sklearn.pipeline import make_pipeline
In [6]: pipeline = make_pipeline(scaler, kmeans)
In [7]: pipeline.fit(samples)
Out[7]: Pipeline(steps=...)
In [8]: labels = pipeline.predict(samples)


Feature standardization improves clustering
In [9]: df = pd.DataFrame({'labels': labels, 'varieties': varieties})
In [10]: ct = pd.crosstab(df['labels'], df['varieties'])
In [11]: print(ct)
varieties Barbera Barolo Grignolino
labels
0 0 59 3
1 48 0 3
2 0 0 65


Without feature standardization was very bad:
varieties Barbera Barolo Grignolino
labels
0 29 13 20
1 0 46 1
2 19 0 50

sklearn preprocessing steps
● StandardScaler is a "preprocessing" step
● MaxAbsScaler and Normalizer are other examples

_______________________________________________________________________________________________________________

Scaling fish data for clustering
You are given an array samples giving measurements of fish. Each row represents an individual fish. The measurements, such as weight in grams, length in centimeters, and the percentage ratio of height to length, have very different scales. In order to cluster this data effectively, you'll need to standardize these features first. In this exercise, you'll build a pipeline to standardize and cluster the data.

These fish measurement data were sourced from the Journal of Statistics Education.

Instructions
100 XP
Import:
make_pipeline from sklearn.pipeline.
StandardScaler from sklearn.preprocessing.
KMeans from sklearn.cluster.
Create an instance of StandardScaler called scaler.
Create an instance of KMeans with 4 clusters called kmeans.
Create a pipeline called pipeline that chains scaler and kmeans. To do this, you just need to pass them in as arguments to make_pipeline().


# Perform the necessary imports
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

# Create scaler: scaler
scaler = StandardScaler()

# Create KMeans instance: kmeans
kmeans = KMeans(n_clusters=4)

# Create pipeline: pipeline
pipeline = make_pipeline(scaler, kmeans)

_______________________________________________________________________________________________________________

Clustering the fish data
You'll now use your standardization and clustering pipeline from the previous exercise to cluster the fish by their measurements, and then create a cross-tabulation to compare the cluster labels with the fish species.

As before, samples is the 2D array of fish measurements. Your pipeline is available as pipeline, and the species of every fish sample is given by the list species.

Instructions
100 XP
Import pandas as pd.
Fit the pipeline to the fish measurements samples.
Obtain the cluster labels for samples by using the .predict() method of pipeline.
Using pd.DataFrame(), create a DataFrame df with two columns named 'labels' and 'species', using labels and species, respectively, for the column values.
Using pd.crosstab(), create a cross-tabulation ct of df['labels'] and df['species'].



# Import pandas
import pandas as pd

# Fit the pipeline to samples
pipeline.fit(samples)

# Calculate the cluster labels: labels
labels = pipeline.predict(samples)

# Create a DataFrame with labels and species as columns: df
df = pd.DataFrame({'labels':labels,'species':species})

# Create crosstab: ct
ct = pd.crosstab(df['labels'],df['species'])

# Display ct
print(ct)


<script.py> output:
    species  Bream  Pike  Roach  Smelt
    labels                            
    0            1     0     19      1
    1           33     0      1      0
    2            0     0      0     13
    3            0    17      0      0

_______________________________________________________________________________________________________________


Clustering stocks using KMeans
In this exercise, you'll cluster companies using their daily stock price movements (i.e. the dollar difference between the closing and opening prices for each trading day). You are given a NumPy array movements of daily price movements from 2010 to 2015 (obtained from Yahoo! Finance), where each row corresponds to a company, and each column corresponds to a trading day.

Some stocks are more expensive than others. To account for this, include a Normalizer at the beginning of your pipeline. The Normalizer will separately transform each company's stock price to a relative scale before the clustering begins.

Note that Normalizer() is different to StandardScaler(), which you used in the previous exercise. While StandardScaler() standardizes features (such as the features of the fish data from the previous exercise) by removing the mean and scaling to unit variance, Normalizer() rescales each sample - here, each company's stock price - independently of the other.

KMeans and make_pipeline have already been imported for you.

Instructions
100 XP
Import Normalizer from sklearn.preprocessing.
Create an instance of Normalizer called normalizer.
Create an instance of KMeans called kmeans with 10 clusters.
Using make_pipeline(), create a pipeline called pipeline that chains normalizer and kmeans.
Fit the pipeline to the movements array.


# Import Normalizer
from sklearn.preprocessing import Normalizer

# Create a normalizer: normalizer
normalizer = Normalizer()

# Create a KMeans model with 10 clusters: kmeans
kmeans = KMeans(n_clusters=10)

# Make a pipeline chaining normalizer and kmeans: pipeline
pipeline = make_pipeline(normalizer, kmeans)

# Fit pipeline to the daily price movements
pipeline.fit(movements)

Great work - you're really getting the hang of this. Now that your pipeline has been set up, you can find out which stocks move together in the next exercise!



_______________________________________________________________________________________________________________

Which stocks move together?
In the previous exercise, you clustered companies by their daily stock price movements. So which company have stock prices that tend to change in the same way? You'll now inspect the cluster labels from your clustering to find out.

Your solution to the previous exercise has already been run. Recall that you constructed a Pipeline pipeline containing a KMeans model and fit it to the NumPy array movements of daily stock movements. In addition, a list companies of the company names is available.

Instructions
100 XP
Import pandas as pd.
Use the .predict() method of the pipeline to predict the labels for movements.
Align the cluster labels with the list of company names companies by creating a DataFrame df with labels and companies as columns. This has been done for you.
Use the .sort_values() method of df to sort the DataFrame by the 'labels' column, and print the result.
Hit 'Submit Answer' and take a moment to see which companies are together in each cluster!


# Import pandas
import pandas as pd

# Predict the cluster labels: labels
labels = pipeline.predict(movements)

# Create a DataFrame aligning labels and companies: df
df = pd.DataFrame({'labels': labels, 'companies': companies})

# Display df sorted by cluster label
print(df.sort_values('labels'))

<script.py> output:
                                 companies  labels
    56                            Wal-Mart       0
    59                               Yahoo       1
    51                   Texas instruments       1
    47                            Symantec       1
    32                                  3M       1
    8                          Caterpillar       1
    30                          MasterCard       1
    24                               Intel       1
    23                                 IBM       1
    20                          Home Depot       1
    13                   DuPont de Nemours       1
    14                                Dell       1
    50  Taiwan Semiconductor Manufacturing       2
    43                                 SAP       2
    33                           Microsoft       2
    17                     Google/Alphabet       2
    0                                Apple       2
    2                               Amazon       2
    18                       Goldman Sachs       3
    1                                  AIG       3
    55                         Wells Fargo       3
    16                   General Electrics       3
    26                      JPMorgan Chase       3
    5                      Bank of America       3
    3                     American express       3
    53                       Valero Energy       4
    57                               Exxon       4
    12                             Chevron       4
    39                              Pfizer       4
    10                      ConocoPhillips       4
    44                        Schlumberger       4
    38                               Pepsi       5
    9                    Colgate-Palmolive       5
    40                      Procter Gamble       5
    28                           Coca Cola       5
    27                      Kimberly-Clark       5
    25                   Johnson & Johnson       5
    31                           McDonalds       5
    41                       Philip Morris       5
    54                            Walgreen       6
    48                              Toyota       7
    45                                Sony       7
    15                                Ford       7
    7                                Canon       7
    35                            Navistar       7
    21                               Honda       7
    22                                  HP       7
    34                          Mitsubishi       7
    11                               Cisco       7
    58                               Xerox       7
    19                     GlaxoSmithKline       8
    52                            Unilever       8
    37                            Novartis       8
    6             British American Tobacco       8
    49                               Total       8
    46                      Sanofi-Aventis       8
    42                   Royal Dutch Shell       8
    4                               Boeing       9
    36                    Northrop Grumman       9
    29                     Lookheed Martin       9
    



=====================================================================================================================================

2
Visualization with hierarchical clustering and t-SNE
0%
In this chapter, you'll learn about two unsupervised learning techniques for data visualization, hierarchical clustering and t-SNE. Hierarchical clustering merges the data samples into ever-coarser clusters, yielding a tree visualization of the resulting cluster hierarchy. t-SNE maps the data samples into 2d space so that the proximity of the samples to one another can be visualized.



Visualisations communicate insight
● "t-SNE" : Creates a 2D map of a dataset (later)
● "Hierarchical clustering" (this video)


A hierarchy of groups
● Groups of living things can form a hierarchy
● Clusters are contained in one another

Eurovision scoring dataset
● Countries gave scores to songs performed at the Eurovision 2016
● 2D array of scores
● Rows are countries, columns are songs


Hierarchical clustering of voting countries


Hierarchical clustering
● Every country begins in a separate cluster
● At each step, the two closest clusters are merged
● Continue until all countries in a single cluster
● This is “agglomerative” hierarchical clustering


The dendrogram of a hierarchical clustering
● Read from the bo!om up
● Vertical lines represent clusters 


Hierarchical clustering with SciPy
In [1]: import matplotlib.pyplot as plt
In [2]: from scipy.cluster.hierarchy import linkage, dendrogram
In [3]: mergings = linkage(samples, method='complete')
In [4]: dendrogram(mergings,
 ...: labels=country_names,
 ...: leaf_rotation=90,
 ...: leaf_font_size=6)
In [5]: plt.show()
● Given samples (the array of scores), and country_names

___________________________________________________________________________________________________________________


Hierarchical clustering of the grain data
In the video, you learned that the SciPy linkage() function performs hierarchical clustering on an array of samples. Use the linkage() function to obtain a hierarchical clustering of the grain samples, and use dendrogram() to visualize the result. A sample of the grain measurements is provided in the array samples, while the variety of each grain sample is given by the list varieties.

Instructions
100 XP
Import:
linkage and dendrogram from scipy.cluster.hierarchy.
matplotlib.pyplot as plt.
Perform hierarchical clustering on samples using the linkage() function with the method='complete' keyword argument. Assign the result to mergings.
Plot a dendrogram using the dendrogram() function on mergings. Specify the keyword arguments labels=varieties, leaf_rotation=90, and leaf_font_size=6.

# Perform the necessary imports
from scipy.cluster.hierarchy import linkage, dendrogram
import matplotlib.pyplot as plt

# Calculate the linkage: mergings
mergings = linkage(samples, method='complete')

# Plot the dendrogram, using varieties as labels
dendrogram(mergings,
           labels=varieties,
           leaf_rotation=90,
           leaf_font_size=6,
)
plt.show()


# Import normalize
from sklearn.preprocessing import normalize

# Normalize the movements: normalized_movements
normalized_movements = normalize(movements)

# Calculate the linkage: mergings
mergings = linkage(normalized_movements, method = 'complete')

# Plot the dendrogram
dendrogram(mergings,
labels = companies,
leaf_rotation = 90,
leaf_font_size=6)
plt.show()

Great work! You can produce great visualizations such as this with hierarchical clustering, but it can be used for more than just visualizations. You'll find out more about this in the next video!



_____________________________________________________________________________________________________________________


Hierarchies of stocks
In chapter 1, you used k-means clustering to cluster companies according to their stock price movements. Now, you'll perform hierarchical clustering of the companies. You are given a NumPy array of price movements movements, where the rows correspond to companies, and a list of the company names companies. SciPy hierarchical clustering doesn't fit into a sklearn pipeline, so you'll need to use the normalize() function from sklearn.preprocessing instead of Normalizer.

linkage and dendrogram have already been imported from sklearn.cluster.hierarchy, and PyPlot has been imported as plt.

Instructions
100 XP
Import normalize from sklearn.preprocessing.
Rescale the price movements for each stock by using the normalize() function on movements.
Apply the linkage() function to normalized_movements, using 'complete' linkage, to calculate the hierarchical clustering. Assign the result to mergings.
Plot a dendrogram of the hierarchical clustering, using the list companies of company names as the labels. In addition, specify the leaf_rotation=90, and leaf_font_size=6 keyword arguments as you did in the previous exercise.


# Import normalize
from sklearn.preprocessing import normalize

# Normalize the movements: normalized_movements
normalized_movements = normalize(movements)

# Calculate the linkage: mergings
mergings = linkage(normalized_movements, method = 'complete')

# Plot the dendrogram
dendrogram(mergings,
labels = companies,
leaf_rotation = 90,
leaf_font_size=6)
plt.show()


___________________________________________________________________________________________________________________

Extracting the cluster labels
In the previous exercise, you saw that the intermediate clustering of the grain samples at height 6 has 3 clusters. Now, use the fcluster() function to extract the cluster labels for this intermediate clustering, and compare the labels with the grain varieties using a cross-tabulation.

The hierarchical clustering has already been performed and mergings is the result of the linkage() function. The list varieties gives the variety of each grain sample.

Instructions
100 XP
Import:
pandas as pd.
fcluster from scipy.cluster.hierarchy.
Perform a flat hierarchical clustering by using the fcluster() function on mergings. Specify a maximum height of 6 and the keyword argument criterion='distance'.
Create a DataFrame df with two columns named 'labels' and 'varieties', using labels and varieties, respectively, for the column values. This has been done for you.
Create a cross-tabulation ct between df['labels'] and df['varieties'] to count the number of times each grain variety coincides with each cluster label.


# Perform the necessary imports
import pandas as pd
from scipy.cluster.hierarchy import fcluster

# Use fcluster to extract labels: labels
labels = fcluster(mergings, 6, criterion='distance')

# Create a DataFrame with labels and varieties as columns: df
df = pd.DataFrame({'labels': labels, 'varieties': varieties})

# Create crosstab: ct
ct = pd.crosstab(df['labels'], df['varieties'])

# Display ct
print(ct)


<script.py> output:
    varieties  Canadian wheat  Kama wheat  Rosa wheat
    labels                                           
    1                      14           3           0
    2                       0           0          14
    3                       0          11           0


Fantastic - you've now mastered the fundamentals of k-Means and agglomerative hierarchical clustering. Next, you'll learn about t-SNE, which is a powerful tool for visualizing high dimensional data.


___________________________________________________________________________________________________________________


Cluster labels in
hierarchical clustering


Cluster labels in hierarchical clustering
● Not only a visualisation tool!
● Cluster labels at any intermediate stage can be recovered
● For use in e.g. cross-tabulations


Intermediate clusterings & height on dendrogram
● E.g. at height 15: Bulgaria, Cyprus, Greece are one cluster
● Russia and Moldova are another
● Armenia in a cluster on its own



Dendrograms show cluster distances
● Height on dendrogram = distance between merging clusters
● E.g. clusters with only Cyprus and Greece had distance approx. 6
● This new cluster distance approx. 12 from cluster with only Bulgaria 



Intermediate clusterings & height on dendrogram
● Height on dendrogram specifies max. distance between merging clusters
● Don't merge clusters further apart than this (e.g. 15)


Distance between clusters
● Defined by a "linkage method"
● Specified via method parameter, e.g. linkage(samples, method="complete")
● In "complete" linkage: distance between clusters is max. distance between
their samples
● Different linkage method, different hierarchical clustering!


Extracting cluster labels
● Use the fcluster method
● Returns a NumPy array of cluster labels


Extracting cluster labels using fcluster
In [1]: from scipy.cluster.hierarchy import linkage
In [2]: mergings = linkage(samples, method='complete')
In [3]: from scipy.cluster.hierarchy import fcluster
In [4]: labels = fcluster(mergings, 15, criterion='distance')
In [5]: print(labels)
[ 9 8 11 20 2 1 17 14 ... ]



Aligning cluster labels with country names
● Given a list of strings country_names
In [1]: import pandas as pd
In [2]: pairs = pd.DataFrame({'labels': labels,
 ...: 'countries': country_names})
In [3]: print(pairs.sort_values('labels'))
 countries labels
5 Belarus 1
40 Ukraine 1
17 Georgia 1
...
36 Spain 5
8 Bulgaria 6
19 Greece 6
10 Cyprus 6
28 Moldova 7 


___________________________________________________________________________________________________________________

Which clusters are closest?
In the video, you learned that the linkage method defines how the distance between clusters is measured. In complete linkage, the distance between clusters is the distance between the furthest points of the clusters. In single linkage, the distance between clusters is the distance between the closest points of the clusters.

Consider the three clusters in the diagram. Which of the following statements are true?

A. In single linkage, cluster 3 is the closest to cluster 2.

B. In complete linkage, cluster 1 is the closest to cluster 2.



________________________________________________________________________________________________________________

Different linkage, different hierarchical clustering!
In the video, you saw a hierarchical clustering of the voting countries at the Eurovision song contest using 'complete' linkage. Now, perform a hierarchical clustering of the voting countries with 'single' linkage, and compare the resulting dendrogram with the one in the video. Different linkage, different hierarchical clustering!

You are given an array samples. Each row corresponds to a voting country, and each column corresponds to a performance that was voted for. The list country_names gives the name of each voting country. This dataset was obtained from Eurovision.

Instructions
100 XP
Import:
linkage and dendrogram from scipy.cluster.hierarchy.
matplotlib.pyplot as plt.
Perform hierarchical clustering on samples using the linkage() function with the method='single' keyword argument. Assign the result to mergings.
Plot a dendrogram of the hierarchical clustering, using the list country_names as the labels. In addition, specify the leaf_rotation=90, and leaf_font_size=6 keyword arguments as you have done earlier.







___________________________________________________________________________________________________________________

t-SNE for
2-dimensional
maps


t-SNE for 2-dimensional maps
● t-SNE = “t-distributed stochastic neighbor embedding”
● Maps samples to 2D space (or 3D)
● Map approximately preserves nearness of samples
● Great for inspecting datasets


t-SNE on the iris dataset
● Iris dataset has 4 measurements, so samples are 4-dimensional
● t-SNE maps samples to 2D space
● t-SNE didn't know that there were different species
● ... yet kept the species mostly separate


Interpreting t-SNE sca!er plots
● “versicolor” and “virginica” harder to distinguish from one another
● Consistent with k-means inertia plot: could argue for 2 clusters, or
for 3


t-SNE in sklearn
In [1]: print(samples)
[[ 5. 3.3 1.4 0.2]
 [ 5. 3.5 1.3 0.3]
 [ 4.9 2.4 3.3 1. ]
 [ 6.3 2.8 5.1 1.5]
 ...
 [ 4.9 3.1 1.5 0.1]]
In [2]: print(species)
[0, 0, 1, 2, ..., 0]
● 2D NumPy array samples
● List species giving species of labels as number (0, 1, or 2)



t-SNE in sklearn
In [3]: import matplotlib.pyplot as plt
In [4]: from sklearn.manifold import TSNE
In [5]: model = TSNE(learning_rate=100)
In [5]: transformed = model.fit_transform(samples)
In [6]: xs = transformed[:,0]
In [7]: ys = transformed[:,1]
In [8]: plt.scatter(xs, ys, c=species)
In [9]: plt.show()



t-SNE has only fit_transform()
● Has a fit_transform() method
● Simultaneously fits the model and transforms the data
● Has no separate fit() or transform() methods
● Can’t extend the map to include new data samples
● Must start over each time!



t-SNE learning rate
● Choose learning rate for the dataset
● Wrong choice: points bunch together
● Try values between 50 and 200



Different every time
● t-SNE features are different every time
● Piedmont wines, 3 runs, 3 different sca!er plots!
● … however: The wine varieties (=colors) have same position relative to
one another
_________________________________________________________________________________________________________________________

t-SNE visualization of grain dataset
In the video, you saw t-SNE applied to the iris dataset. In this exercise, you'll apply t-SNE to the grain samples data and inspect the resulting t-SNE features using a scatter plot. You are given an array samples of grain samples and a list variety_numbers giving the variety number of each grain sample.

Instructions
100 XP
Import TSNE from sklearn.manifold.
Create a TSNE instance called model with learning_rate=200.
Apply the .fit_transform() method of model to samples. Assign the result to tsne_features.
Select the column 0 of tsne_features. Assign the result to xs.
Select the column 1 of tsne_features. Assign the result to ys.
Make a scatter plot of the t-SNE features xs and ys. To color the points by the grain variety, specify the additional keyword argument c=variety_numbers


# Import TSNE
from sklearn.manifold import TSNE

# Create a TSNE instance: model
model = TSNE(learning_rate=200)

# Apply fit_transform to samples: tsne_features
tsne_features = model.fit_transform(samples)

# Select the 0th feature: xs
xs = tsne_features[:,0]

# Select the 1st feature: ys
ys = tsne_features[:,1]

# Scatter plot, coloring by variety_numbers
plt.scatter(xs, ys, c=variety_numbers)
plt.show()


_________________________________________________________________________________________________________________________
A t-SNE map of the stock market
t-SNE provides great visualizations when the individual samples can be labeled. In this exercise, you'll apply t-SNE to the company stock price data. A scatter plot of the resulting t-SNE features, labeled by the company names, gives you a map of the stock market! The stock price movements for each company are available as the array normalized_movements (these have already been normalized for you). The list companies gives the name of each company. PyPlot (plt) has been imported for you.

Instructions
100 XP
Import TSNE from sklearn.manifold.
Create a TSNE instance called model with learning_rate=50.
Apply the .fit_transform() method of model to normalized_movements. Assign the result to tsne_features.
Select column 0 and column 1 of tsne_features.
Make a scatter plot of the t-SNE features xs and ys. Specify the additional keyword argument alpha=0.5.
Code to label each point with its company name has been written for you using plt.annotate(), so just hit 'Submit Answer' to see the visualization!

# Import TSNE 
from sklearn.manifold import TSNE 

# Create a TSNE instance: model
model = TSNE(learning_rate=50)

# Apply fit_transform to normalized_movements: tsne_features
tsne_features = model.fit_transform(normalized_movements)

# Select the 0th feature: xs
xs = tsne_features[:,0]

# Select the 1th feature: ys
ys = tsne_features[:,1]

# Scatter plot
plt.scatter(xs, ys, alpha=0.5)

# Annotate the points
for x, y, company in zip(xs, ys, companies):
    plt.annotate(company, (x, y), fontsize=5, alpha=0.75)
plt.show()





=====================================================================================================================================

3
Decorrelating your data and dimension reduction
0%
Dimension reduction summarizes a dataset using its common occuring patterns. In this chapter, you'll learn about the most fundamental of dimension reduction techniques, "Principal Component Analysis" ("PCA"). PCA is often used before supervised learning to improve model performance and generalization. It can also be useful for unsupervised learning. For example, you'll employ a variant of PCA will allow you to cluster Wikipedia articles by their content!


_________________________________________________________________________________________________________________________

Visualizing the
PCA transformation


Dimension reduction
● More efficient storage and computation
● Remove less-informative "noise" features
● ... which cause problems for prediction tasks, e.g.
classification, regression


Principal Component Analysis
● PCA = "Principal Component Analysis"
● Fundamental dimension reduction technique
● First step "decorrelation" (considered here)
● Second step reduces dimension (considered later)



PCA aligns data with axes
● Rotates data samples to be aligned with axes
● Shi"s data samples so they have mean 0
● No information is lost


PCA follows the fit/transform pa!ern
● PCA a scikit-learn component like KMeans or
StandardScaler
● fit() learns the transformation from given data
● transform() applies the learned transformation
● transform() can also be applied to new data



Using scikit-learn PCA
In [1]: print(samples)
[[ 2.8 3.92]
 [ 2.65 3.4 ]
 ...
 [ 2.05 1.6 ]]
In [2]: from sklearn.decomposition import PCA
In [3]: model = PCA()
In [4]: model.fit(samples)
Out[4]: PCA(copy=True, ...)
In [5]: transformed = model.transform(samples)
● samples = array of two wine features (total_phenols & od280)



PCA features
In [6]: print(transformed)
[[ 1.32771994e+00 4.51396070e-01]
 [ 8.32496068e-01 2.33099664e-01]
 ...
 [ -9.33526935e-01 -4.60559297e-01]]
● Rows of transformed correspond to samples
● Columns of transformed are the "PCA features"
● Row gives PCA feature values of corresponding sample


PCA features are not correlated
● Features of dataset are o"en correlated, e.g. total_phenols and od280
● PCA aligns the data with axes
● Resulting PCA features are not linearly correlated ("decorrelation")

Pearson correlation
● Measures linear correlation of features
● Value between -1 and 1
● Value of 0 means no linear correlation


Principal components
● "Principal components" = directions of variance
● PCA aligns principal components with the axes
● Available as components_ a#ribute of PCA object
● Each row defines displacement from mean

_________________________________________________________________________________________________________________________

Correlated data in nature
You are given an array grains giving the width and length of samples of grain. You suspect that width and length will be correlated. To confirm this, make a scatter plot of width vs length and measure their Pearson correlation.

Instructions
100 XP
Import:
matplotlib.pyplot as plt.
pearsonr from scipy.stats.
Assign column 0 of grains to width and column 1 of grains to length.
Make a scatter plot with width on the x-axis and length on the y-axis.
Use the pearsonr() function to calculate the Pearson correlation of width and length.


# Perform the necessary imports
import matplotlib.pyplot as plt
from scipy.stats import pearsonr

# Assign the 0th column of grains: width
width = grains[:,0]

# Assign the 1st column of grains: length
length = grains[:,1]

# Scatter plot width vs length
plt.scatter(width, length)
plt.axis('equal')
plt.show()

# Calculate the Pearson correlation
correlation, pvalue = pearsonr(width, length)

# Display the correlation
print(correlation)

_________________________________________________________________________________________________________________________

Decorrelating the grain measurements with PCA
You observed in the previous exercise that the width and length measurements of the grain are correlated. Now, you'll use PCA to decorrelate these measurements, then plot the decorrelated points and measure their Pearson correlation.

Instructions
100 XP
Import PCA from sklearn.decomposition.
Create an instance of PCA called model.
Use the .fit_transform() method of model to apply the PCA transformation to grains. Assign the result to pca_features.
The subsequent code to extract, plot, and compute the Pearson correlation of the first two columns pca_features has been written for you, so hit 'Submit Answer' to see the result!

# Import PCA
from sklearn.decomposition import PCA

# Create PCA instance: model
model = PCA()

# Apply the fit_transform method of model to grains: pca_features
pca_features = model.fit_transform(grains)

# Assign 0th column of pca_features: xs
xs = pca_features[:,0]

# Assign 1st column of pca_features: ys
ys = pca_features[:,1]

# Scatter plot xs vs ys
plt.scatter(xs, ys)
plt.axis('equal')
plt.show()

# Calculate the Pearson correlation of xs and ys
correlation, pvalue = pearsonr(xs, ys)

# Display the correlation
print(correlation)







_________________________________________________________________________________________________________________________


Intrinsic dimension


Intrinsic dimension of a flight path
● 2 features: longitude and latitude at points along a flight path
● Dataset appears to be 2-dimensional
● But can approximate using one feature: displacement along flight path
● Is intrinsically 1-dimensional

Intrinsic dimension
● Intrinsic dimension = number of features needed to
approximate the dataset
● Essential idea behind dimension reduction
● What is the most compact representation of the samples?
● Can be detected with PCA


Versicolor dataset
● "versicolor", one of the iris species
● Only 3 features: sepal length, sepal width, and petal width
● Samples are points in 3D space


Versicolor dataset has intrinsic dimension 2
● Samples lie close to a flat 2-dimensional sheet
● So can be approximated using 2 features


PCA identifies intrinsic dimension
● Sca#er plots work only if samples have 2 or 3 features
● PCA identifies intrinsic dimension when samples have any
number of features
● Intrinsic dimension = number of PCA features with significant
variance


PCA of the versicolor samples


PCA features are ordered by variance descending


Variance and intrinsic dimension
● Intrinsic dimension is number of PCA features with significant variance
● In our example: the first two PCA features
● So intrinsic dimension is 2



Plo!ing the variances of PCA features
In [1]: import matplotlib.pyplot as plt
In [2]: from sklearn.decomposition import PCA
In [3]: pca = PCA()
In [4]: pca.fit(samples)
Out[4]: PCA(copy=True, ... )
In [5]: features = range(pca.n_components_)



Plo!ing the variances of PCA features
In [6]: plt.bar(features, pca.explained_variance_)
In [7]: plt.xticks(features)
In [8]: plt.ylabel('variance')
In [9]: plt.xlabel('PCA feature')
In [10]: plt.show()


Intrinsic dimension can be ambiguous
● Intrinsic dimension is an idealization
● … there is not always one correct answer!
● Piedmont wines: could argue for 2, or for 3, or more

_________________________________________________________________________________________________________________________

The first principal component
The first principal component of the data is the direction in which the data varies the most. In this exercise, your job is to use PCA to find the first principal component of the length and width measurements of the grain samples, and represent it as an arrow on the scatter plot.

The array grains gives the length and width of the grain samples. PyPlot (plt) and PCA have already been imported for you.

Instructions
100 XP
Make a scatter plot of the grain measurements. This has been done for you.
Create a PCA instance called model.
Fit the model to the grains data.
Extract the coordinates of the mean of the data using the .mean_ attribute of model.
Get the first principal component of model using the .components_[0,:] attribute.
Plot the first principal component as an arrow on the scatter plot, using the plt.arrow() function. You have to specify the first two arguments - mean[0] and mean[1].



# Make a scatter plot of the untransformed points
plt.scatter(grains[:,0], grains[:,1])

# Create a PCA instance: model
model = PCA()

# Fit model to points
model.fit(grains)

# Get the mean of the grain samples: mean
mean = model.mean_

# Get the first principal component: first_pc
first_pc = model.components_[0,:]

# Plot first_pc as an arrow, starting at mean
plt.arrow(mean[0], mean[1], first_pc[0], first_pc[1], color='red', width=0.01)

# Keep axes on same scale
plt.axis('equal')
plt.show()

_________________________________________________________________________________________________________________________

Variance of the PCA features
The fish dataset is 6-dimensional. But what is its intrinsic dimension? Make a plot of the variances of the PCA features to find out. As before, samples is a 2D array, where each row represents a fish. You'll need to standardize the features first.

Instructions
100 XP
Create an instance of StandardScaler called scaler.
Create a PCA instance called pca.
Use the make_pipeline() function to create a pipeline chaining scaler and pca.
Use the .fit() method of pipeline to fit it to the fish samples samples.
Extract the number of components used using the .n_components_ attribute of pca. Place this inside a range() function and store the result as features.
Use the plt.bar() function to plot the explained variances, with features on the x-axis and pca.explained_variance_ on the y-axis.

# Perform the necessary imports
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
import matplotlib.pyplot as plt

# Create scaler: scaler
scaler = StandardScaler()

# Create a PCA instance: pca
pca = PCA()

# Create pipeline: pipeline
pipeline = make_pipeline(scaler, pca)

# Fit the pipeline to 'samples'
pipeline.fit(samples)

# Plot the explained variances
features = range(pca.n_components_)
plt.bar(features, pca.explained_variance_)
plt.xlabel('PCA feature')
plt.ylabel('variance')
plt.xticks(features)
plt.show()

_________________________________________________________________________________________________________________________

Dimension reduction
with PCA


Dimension reduction
● Represents same data, using less features
● Important part of machine-learning pipelines
● Can be performed using PCA


Dimension reduction with PCA
● PCA features are in decreasing order of variance
● Assumes the low variance features are "noise"
● … and high variance features are informative



Dimension reduction with PCA
● Specify how many features to keep
● E.g. PCA(n_components=2)
● Keeps the first 2 PCA features
● Intrinsic dimension is a good choice


Dimension reduction of iris dataset
In [1]: from sklearn.decomposition import PCA
In [2]: pca = PCA(n_components=2)
In [3]: pca.fit(samples)
Out[3]: PCA(copy=True, ... )
In [4]: transformed = pca.transform(samples)
In [5]: print(transformed.shape)
(150, 2)

● samples = array of iris measurements (4 features)
● species = list of iris species numbers



Iris dataset in 2 dimensions
● PCA has reduced the dimension to 2
● Retained the 2 PCA features with highest variance
● Important information preserved: species remain distinct
In [6]: import matplotlib.pyplot as plt
In [7]: xs = transformed[:,0]
 ...: ys = transformed[:,1]
In [8]: plt.scatter(xs, ys, c=species)
 ...: plt.show()
 
 
 Dimension reduction with PCA
● Discards low variance PCA features
● Assumes the high variance features are informative
● Assumption typically holds in practice (e.g. for iris)


Word frequency arrays
● Rows represent documents, columns represent words
● Entries measure presence of each word in each document
● ... measure using "tf-idf" (more later)


Sparse arrays and csr_matrix
● Array is "sparse": most entries are zero
● Can use scipy.sparse.csr_matrix instead of NumPy array
● csr_matrix remembers only the non-zero entries (saves space!)
 

TruncatedSVD and csr_matrix
In [1]: from sklearn.decomposition import TruncatedSVD
In [2]: model = TruncatedSVD(n_components=3)
In [3]: model.fit(documents) # documents is csr_matrix
Out[3]: TruncatedSVD(algorithm='randomized', ... )
In [4]: transformed = model.transform(documents)
● scikit-learn PCA doesn't support csr_matrix
● Use scikit-learn TruncatedSVD instead
● Performs same transformation
_______________________________________________________________________________________________________________________________

Dimension reduction of the fish measurements
In a previous exercise, you saw that 2 was a reasonable choice for the "intrinsic dimension" of the fish measurements. Now use PCA for dimensionality reduction of the fish measurements, retaining only the 2 most important components.

The fish measurements have already been scaled for you, and are available as scaled_samples.

Instructions
100 XP
Import PCA from sklearn.decomposition.
Create a PCA instance called pca with n_components=2.
Use the .fit() method of pca to fit it to the scaled fish measurements scaled_samples.
Use the .transform() method of pca to transform the scaled_samples. Assign the result to pca_features.

# Import PCA
from sklearn.decomposition import PCA

# Create a PCA model with 2 components: pca
pca = PCA(n_components=2)

# Fit the PCA instance to the scaled samples
pca.fit(scaled_samples)

# Transform the scaled samples: pca_features
pca_features = pca.transform(scaled_samples)

# Print the shape of pca_features
print(pca_features.shape)


_________________________________________________________________________________________________________


A tf-idf word-frequency array
In this exercise, you'll create a tf-idf word frequency array for a toy collection of documents. For this, use the TfidfVectorizer from sklearn. It transforms a list of documents into a word frequency array, which it outputs as a csr_matrix. It has fit() and transform() methods like other sklearn objects.

You are given a list documents of toy documents about pets. Its contents have been printed in the IPython Shell.

Instructions
100 XP
Import TfidfVectorizer from sklearn.feature_extraction.text.
Create a TfidfVectorizer instance called tfidf.
Apply .fit_transform() method of tfidf to documents and assign the result to csr_mat. This is a word-frequency array in csr_matrix format.
Inspect csr_mat by calling its .toarray() method and printing the result. This has been done for you.
The columns of the array correspond to words. Get the list of words by calling the .get_feature_names() method of tfidf, and assign the result to words.

# Import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

# Create a TfidfVectorizer: tfidf
tfidf = TfidfVectorizer()

# Apply fit_transform to document: csr_mat
csr_mat = tfidf.fit_transform(documents)

# Print result of toarray() method
print(csr_mat.toarray())

# Get the words: words
words = tfidf.get_feature_names()

# Print words
print(words)

<script.py> output:
    [[ 0.51785612  0.          0.          0.68091856  0.51785612  0.        ]
     [ 0.          0.          0.51785612  0.          0.51785612  0.68091856]
     [ 0.51785612  0.68091856  0.51785612  0.          0.          0.        ]]
    ['cats', 'chase', 'dogs', 'meow', 'say', 'woof']

_________________________________________________________________________________________________________

Clustering Wikipedia part I
You saw in the video that TruncatedSVD is able to perform PCA on sparse arrays in csr_matrix format, such as word-frequency arrays. Combine your knowledge of TruncatedSVD and k-means to cluster some popular pages from Wikipedia. In this exercise, build the pipeline. In the next exercise, you'll apply it to the word-frequency array of some Wikipedia articles.

Create a Pipeline object consisting of a TruncatedSVD followed by KMeans. (This time, we've precomputed the word-frequency matrix for you, so there's no need for a TfidfVectorizer).

The Wikipedia dataset you will be working with was obtained from here.

Instructions
100 XP
Import:
TruncatedSVD from sklearn.decomposition.
KMeans from sklearn.cluster.
make_pipeline from sklearn.pipeline.
Create a TruncatedSVD instance called svd with n_components=50.
Create a KMeans instance called kmeans with n_clusters=6.
Create a pipeline called pipeline consisting of svd and kmeans.

# Perform the necessary imports
from sklearn.decomposition import TruncatedSVD
from sklearn.cluster import KMeans
from sklearn.pipeline import make_pipeline

# Create a TruncatedSVD instance: svd
svd = TruncatedSVD(n_components = 50)

# Create a KMeans instance: kmeans
kmeans = KMeans(n_clusters=6)

# Create a pipeline: pipeline
pipeline = make_pipeline(svd, kmeans)
_________________________________________________________________________________________________________

Clustering Wikipedia part II
It is now time to put your pipeline from the previous exercise to work! You are given an array articles of tf-idf word-frequencies of some popular Wikipedia articles, and a list titles of their titles. Use your pipeline to cluster the Wikipedia articles.

A solution to the previous exercise has been pre-loaded for you, so a Pipeline pipeline chaining TruncatedSVD with KMeans is available.

Instructions
100 XP
Import pandas as pd.
Fit the pipeline to the word-frequency array articles.
Predict the cluster labels.
Align the cluster labels with the list titles of article titles by creating a DataFrame df with labels and titles as columns. This has been done for you.
Use the .sort_values() method of df to sort the DataFrame by the 'label' column, and print the result.
Hit 'Submit Answer' and take a moment to investigate your amazing clustering of Wikipedia pages!


# Import pandas
import pandas as pd

# Fit the pipeline to articles
pipeline.fit(articles)

# Calculate the cluster labels: labels
labels = pipeline.predict(articles)

# Create a DataFrame aligning labels and titles: df
df = pd.DataFrame({'label': labels, 'article': titles})

# Display df sorted by cluster label
print(df.sort_values('label'))



<script.py> output:
                                              article  label
    34                             Zlatan Ibrahimović      0
    31                              Cristiano Ronaldo      0
    35                Colombia national football team      0
    36              2014 FIFA World Cup qualification      0
    37                                       Football      0
    38                                         Neymar      0
    39                                  Franck Ribéry      0
    33                                 Radamel Falcao      0
    30                  France national football team      0
    32                                   Arsenal F.C.      0
    21                             Michael Fassbender      1
    22                              Denzel Washington      1
    23                           Catherine Zeta-Jones      1
    24                                   Jessica Biel      1
    25                                  Russell Crowe      1
    26                                     Mila Kunis      1
    20                                 Angelina Jolie      1
    29                               Jennifer Aniston      1
    27                                 Dakota Fanning      1
    28                                  Anne Hathaway      1
    50                                   Chad Kroeger      2
    58                                         Sepsis      2
    51                                     Nate Ruess      2
    52                                     The Wanted      2
    53                                   Stevie Nicks      2
    54                                 Arctic Monkeys      2
    55                                  Black Sabbath      2
    56                                       Skrillex      2
    57                          Red Hot Chili Peppers      2
    59                                    Adam Levine      2
    44                                           Gout      3
    49                                       Lymphoma      3
    48                                     Gabapentin      3
    47                                          Fever      3
    46                                     Prednisone      3
    45                                    Hepatitis C      3
    43                                       Leukemia      3
    42                                    Doxycycline      3
    41                                    Hepatitis B      3
    40                                    Tonsillitis      3
    19  2007 United Nations Climate Change Conference      4
    18  2010 United Nations Climate Change Conference      4
    10                                 Global warming      4
    11       Nationally Appropriate Mitigation Action      4
    12                                   Nigel Lawson      4
    13                               Connie Hedegaard      4
    14                                 Climate change      4
    15                                 Kyoto Protocol      4
    16                                        350.org      4
    17  Greenhouse gas emissions by the United States      4
    1                                  Alexa Internet      5
    2                               Internet Explorer      5
    3                                     HTTP cookie      5
    4                                   Google Search      5
    8                                         Firefox      5
    6                     Hypertext Transfer Protocol      5
    7                                   Social search      5
    9                                        LinkedIn      5
    5                                          Tumblr      5
    0                                        HTTP 404      5





=====================================================================================================================================


4
Discovering interpretable features
0%
In this chapter, you'll learn about a dimension reduction technique called "Non-negative matrix factorization" ("NMF") that expresses samples as combinations of interpretable parts. For example, it expresses documents as combinations of topics, and images in terms of commonly occurring visual patterns. You'll also learn to use NMF to build recommender systems that can find you similar articles to read, or musical artists that match your listening history!


_________________________________________________________________________________________________________

Non-negative
 matrix factorization (NMF)
 
Non-negative matrix factorization
● NMF = "non-negative matrix factorization"
● Dimension reduction technique
● NMF models are interpretable (unlike PCA)
● Easy to interpret means easy to explain!
● However, all sample features must be non-negative (>= 0)


Interpretable parts
● NMF expresses documents as combinations of topics (or
"themes")
python
r
program
function
method
…
data
analysis
cluster
statistics
mean
…
DataCamp
DataCamp is the
first and foremost
leader in Data
Science
Education offering
skill-based
training,
pioneering
technical
innovation
…
learn
teaching
lesson
lessons
cours



Interpretable parts
● NMF expresses images as combinations of pa!erns


Using scikit-learn NMF
● Follows fit() / transform() pa!ern
● Must specify number of components e.g. NMF(n_components=2)
● Works with NumPy arrays and with csr_matrix


Example word-frequency array
● Word frequency array, 4 words, many documents
● Measure presence of words in each document using "tf-idf"
● "tf" = frequency of word in document
● "idf" reduces influence of frequent words


Example usage of NMF
In [1]: from sklearn.decomposition import NMF
In [2]: model = NMF(n_components=2)
In [3]: model.fit(samples)
Out[3]: NMF(alpha=0.0, ... )
In [4]: nmf_features = model.transform(samples)
● samples is the word-frequency array

 
 
NMF components
● NMF has components
● ... just like PCA has principal components
● Dimension of components = dimension of samples
● Entries are non-negative
In [5]: print(model.components_)
[[ 0.01 0. 2.13 0.54]
 [ 0.99 1.47 0. 0.5 ]]
 
 
 NMF features
● NMF feature values are non-negative
● Can be used to reconstruct the samples
● ... combine feature values with components
In [6]: print(nmf_features)
[[ 0. 0.2 ]
 [ 0.19 0. ]
 ...
 [ 0.15 0.12]]
 
 
 Reconstruction of a sample
 0.15 *
+ 0.12 *
[ 0.1203 0.1764 0.3195 0.141 ]
In [7]: print(samples[i,:])
[ 0.12 0.18 0.32 0.14]
In [8]: print(nmf_features[i,:])
[ 0.15 0.12]
[[ 0.01 0. 2.13 0.54 ]
 [ 0.99 1.47 0. 0.5 ]]
model.components_
reconstruction of sample


Sample reconstruction
● Multiply components by feature values, and add up
● Can also be expressed as a product of matrices
● This is the "Matrix Factorization" in "NMF"


NMF fits to non-negative data, only
● Word frequencies in each document
● Images encoded as arrays
● Audio spectrograms
● Purchase histories on e-commerce sites
● … and many more!

_________________________________________________________________________________________________________

NMF applied to Wikipedia articles
In the video, you saw NMF applied to transform a toy word-frequency array. Now it's your turn to apply NMF, this time using the tf-idf word-frequency array of Wikipedia articles, given as a csr matrix articles. Here, fit the model and transform the articles. In the next exercise, you'll explore the result.

Instructions
100 XP
Import NMF from sklearn.decomposition.
Create an NMF instance called model with 6 components.
Fit the model to the word count data articles.
Use the .transform() method of model to transform articles, and assign the result to nmf_features.
Print nmf_features to get a first idea what it looks like.

# Import NMF
from sklearn.decomposition import NMF

# Create an NMF instance: model
model = NMF(n_components = 6)

# Fit the model to articles
model.fit(articles)

# Transform the articles: nmf_features
nmf_features = model.transform(articles)

# Print the NMF features
print(nmf_features)


<script.py> output:
    [[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
        0.00000000e+00   4.40537550e-01]
     [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
        0.00000000e+00   5.66697169e-01]
     [  3.82028842e-03   0.00000000e+00   0.00000000e+00   0.00000000e+00
        0.00000000e+00   3.98711863e-01]
     [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
        0.00000000e+00   3.81802473e-01]
     [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
        0.00000000e+00   4.85595702e-01]
     [  1.29282152e-02   1.37897342e-02   7.76329173e-03   3.34408940e-02
        0.00000000e+00   3.34576667e-01]
     [  0.00000000e+00   0.00000000e+00   2.06743313e-02   0.00000000e+00
        6.04483999e-03   3.59120109e-01]
     [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
        0.00000000e+00   4.91056189e-01]
     [  1.54263607e-02   1.42824958e-02   3.76634980e-03   2.37057194e-02
        2.62620104e-02   4.80853190e-01]
     [  1.11731168e-02   3.13694987e-02   3.09488233e-02   6.56848407e-02
        1.96677357e-02   3.38344150e-01]
     [  0.00000000e+00   0.00000000e+00   5.30724169e-01   0.00000000e+00
        2.83680833e-02   0.00000000e+00]
     [  0.00000000e+00   0.00000000e+00   3.56512510e-01   0.00000000e+00
        0.00000000e+00   0.00000000e+00]
     [  1.20119326e-02   6.50071131e-03   3.12248026e-01   6.09631371e-02
        1.13861828e-02   1.92634590e-02]
     [  3.93459652e-03   6.24467948e-03   3.42376317e-01   1.10743717e-02
        0.00000000e+00   0.00000000e+00]
     [  4.63790779e-03   0.00000000e+00   4.34918922e-01   0.00000000e+00
        3.84276770e-02   3.08185068e-03]
     [  0.00000000e+00   0.00000000e+00   4.83293440e-01   0.00000000e+00
        0.00000000e+00   0.00000000e+00]
     [  5.64979593e-03   1.83542969e-02   3.76536354e-01   3.25386514e-02
        0.00000000e+00   1.13353377e-02]
     [  0.00000000e+00   0.00000000e+00   4.80918082e-01   0.00000000e+00
        0.00000000e+00   0.00000000e+00]
     [  0.00000000e+00   9.01900981e-03   5.51012866e-01   0.00000000e+00
        0.00000000e+00   0.00000000e+00]
     [  0.00000000e+00   0.00000000e+00   4.65973809e-01   0.00000000e+00
        0.00000000e+00   0.00000000e+00]
     [  0.00000000e+00   1.14085492e-02   2.08657559e-02   5.17649098e-01
        5.81410759e-02   1.37877426e-02]
     [  0.00000000e+00   0.00000000e+00   0.00000000e+00   5.10358757e-01
        0.00000000e+00   0.00000000e+00]
     [  0.00000000e+00   5.60126890e-03   0.00000000e+00   4.22283440e-01
        0.00000000e+00   0.00000000e+00]
     [  0.00000000e+00   0.00000000e+00   0.00000000e+00   4.36651556e-01
        0.00000000e+00   0.00000000e+00]
     [  0.00000000e+00   0.00000000e+00   0.00000000e+00   4.97978339e-01
        0.00000000e+00   0.00000000e+00]
     [  9.88328587e-02   8.60078405e-02   3.91039486e-03   3.80930550e-01
        4.39244169e-04   5.22242083e-03]
     [  0.00000000e+00   0.00000000e+00   0.00000000e+00   5.72039255e-01
        0.00000000e+00   7.13668262e-03]
     [  1.31459990e-02   1.04857558e-02   0.00000000e+00   4.68799007e-01
        0.00000000e+00   1.16329437e-02]
     [  3.84523605e-03   0.00000000e+00   0.00000000e+00   5.75579129e-01
        0.00000000e+00   0.00000000e+00]
     [  2.25229694e-03   1.38742556e-03   0.00000000e+00   5.27825223e-01
        1.20265122e-02   1.49509373e-02]
     [  0.00000000e+00   4.07564238e-01   1.85716220e-03   0.00000000e+00
        2.96611934e-03   4.52407969e-04]
     [  1.53411152e-03   6.08197003e-01   5.22282514e-04   6.24710508e-03
        1.18445178e-03   4.40137175e-04]
     [  5.38783564e-03   2.65027493e-01   5.38515027e-04   1.86883266e-02
        6.38653852e-03   2.90152485e-03]
     [  0.00000000e+00   6.44941315e-01   0.00000000e+00   0.00000000e+00
        0.00000000e+00   0.00000000e+00]
     [  0.00000000e+00   6.08930968e-01   0.00000000e+00   0.00000000e+00
        0.00000000e+00   0.00000000e+00]
     [  0.00000000e+00   3.43698795e-01   0.00000000e+00   0.00000000e+00
        3.97796654e-03   0.00000000e+00]
     [  6.10468529e-03   3.15325246e-01   1.54881377e-02   0.00000000e+00
        5.06246723e-03   4.74412331e-03]
     [  6.47329526e-03   2.13336953e-01   9.49503916e-03   4.56876974e-02
        1.71915205e-02   9.52221342e-03]
     [  7.99094363e-03   4.67613593e-01   0.00000000e+00   2.43369814e-02
        0.00000000e+00   0.00000000e+00]
     [  0.00000000e+00   6.42845443e-01   0.00000000e+00   2.35801383e-03
        0.00000000e+00   0.00000000e+00]
     [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
        4.77082016e-01   0.00000000e+00]
     [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
        4.94255148e-01   0.00000000e+00]
     [  0.00000000e+00   2.99077586e-04   2.14488829e-03   0.00000000e+00
        3.81777901e-01   5.83880177e-03]
     [  0.00000000e+00   0.00000000e+00   0.00000000e+00   5.64564666e-03
        5.42240389e-01   0.00000000e+00]
     [  1.78047619e-03   7.84444104e-04   1.41629141e-02   4.59712971e-04
        4.24301597e-01   0.00000000e+00]
     [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
        5.11390831e-01   0.00000000e+00]
     [  0.00000000e+00   0.00000000e+00   3.28388532e-03   0.00000000e+00
        3.72886108e-01   0.00000000e+00]
     [  0.00000000e+00   2.62094706e-04   3.61107663e-02   2.32283827e-04
        2.30510297e-01   0.00000000e+00]
     [  1.12510199e-02   2.12335653e-03   1.60973854e-02   1.02461297e-02
        3.25460941e-01   3.75942903e-02]
     [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00
        4.18957425e-01   3.57767739e-04]
     [  3.08353063e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00
        0.00000000e+00   0.00000000e+00]
     [  3.68157223e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00
        0.00000000e+00   0.00000000e+00]
     [  3.97926884e-01   2.81714228e-02   3.67015389e-03   1.70027867e-02
        1.95967122e-03   2.11679187e-02]
     [  3.75777630e-01   2.07529067e-03   0.00000000e+00   3.72069359e-02
        0.00000000e+00   5.86024044e-03]
     [  4.38008419e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00
        0.00000000e+00   0.00000000e+00]
     [  4.57860338e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00
        0.00000000e+00   0.00000000e+00]
     [  2.75464779e-01   4.46974310e-03   0.00000000e+00   5.29534527e-02
        0.00000000e+00   1.91029166e-02]
     [  4.45173818e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00
        5.48698118e-03   0.00000000e+00]
     [  2.92727168e-01   1.33670098e-02   1.14264445e-02   1.05175992e-02
        1.87696132e-01   9.24117180e-03]
     [  3.78249406e-01   1.43975990e-02   0.00000000e+00   9.85014498e-02
        1.35900234e-02   0.00000000e+00]]


_________________________________________________________________________________________________________

NMF features of the Wikipedia articles
Now you will explore the NMF features you created in the previous exercise. A solution to the previous exercise has been pre-loaded, so the array nmf_features is available. Also available is a list titles giving the title of each Wikipedia article.

When investigating the features, notice that for both actors, the NMF feature 3 has by far the highest value. This means that both articles are reconstructed using mainly the 3rd NMF component. In the next video, you'll see why: NMF components represent topics (for instance, acting!).

Instructions
100 XP
Import pandas as pd.
Create a DataFrame df from nmf_features using pd.DataFrame(). Set the index to titles using index=titles.
Use the .loc[] accessor of df to select the row with title 'Anne Hathaway', and print the result. These are the NMF features for the article about the actress Anne Hathaway.
Repeat the last step for 'Denzel Washington' (another actor).


# Import pandas
import pandas as pd

# Create a pandas DataFrame: df
df = pd.DataFrame(nmf_features, index=titles)

# Print the row for 'Anne Hathaway'
print(df.loc['Anne Hathaway'])

# Print the row for 'Denzel Washington'
print(df.loc['Denzel Washington'])

<script.py> output:
    0    0.003845
    1    0.000000
    2    0.000000
    3    0.575711
    4    0.000000
    5    0.000000
    Name: Anne Hathaway, dtype: float64
    0    0.000000
    1    0.005601
    2    0.000000
    3    0.422380
    4    0.000000
    5    0.000000
    Name: Denzel Washington, dtype: float64


Great work! Notice that for both actors, the NMF feature 3 has by far the highest value. This means that both articles are reconstructed using mainly the 3rd NMF component. In the next video, you'll see why: NMF components represent topics (for instance, acting!).

_________________________________________________________________________________________________________

NMF reconstructs samples
In this exercise, you'll check your understanding of how NMF reconstructs samples from its components using the NMF feature values. On the right are the components of an NMF model. If the NMF feature values of a sample are [2, 1], then which of the following is most likely to represent the original sample? A pen and paper will help here! You have to apply the same technique Ben used in the video to reconstruct the sample [0.1203 0.1764 0.3195 0.141].

[[ 1.   0.5  0. ]
 [ 0.2  0.1  2.1]]
 
Answer:  [2.2, 1.0, 2.0].
 
 

_________________________________________________________________________________________________________


NMF learns
interpretable parts


Example: NMF learns interpretable parts
● Word-frequency array articles (tf-idf)
● 20,000 scientific articles (rows)
● 800 words (columns)


Applying NMF to the articles
In [1]: print(articles.shape)
(20000, 800)
In [2]: from sklearn.decomposition import NMF
In [3]: nmf = NMF(n_components=10)
In [4]: nmf.fit(articles)
Out[4]: NMF(alpha=0.0, ... )
In [5]: print(nmf.components_.shape)
(10, 800)


NMF components are topics aardvark arch . . .zebra
nmf.components_
 0.1 0.0 . . . 0.1
top words:
species 2.95
plant 1.05
plants 0.78
genetic 0.58
evolution 0.53
life 0



NMF components
● For documents:
● NMF components represent topics
● NMF features combine topics into documents
● For images, NMF components are parts of images


Grayscale images
● "Grayscale" image = no colors, only shades of gray
● Measure pixel brightness
● Represent with value between 0 and 1 (0 is black)
● Convert to 2D array


Grayscale image example
● An 8x8 grayscale image of the moon, written as an array


Grayscale images as flat arrays
● Enumerate the entries
● Row-by-row
● From le" to right

Encoding a collection of images
● Collection of images of the same size
● Encode as 2D array
● Each row corresponds to an image
● Each column corresponds to a pixel
● ... can apply NMF


Visualizing samples
In [1]: print(sample)
[ 0. 1. 0.5 1. 0. 1. ]
In [2]: bitmap = sample.reshape((2, 3))
In [3]: print(bitmap)
[[ 0. 1. 0.5]
 [ 1. 0. 1. ]]
In [4]: from matplotlib import pyplot as plt
In [5]: plt.imshow(bitmap, cmap='gray', interpolation='nearest')
In [6]: plt.show()
_________________________________________________________________________________________________________

NMF learns topics of documents
In the video, you learned when NMF is applied to documents, the components correspond to topics of documents, and the NMF features reconstruct the documents from the topics. Verify this for yourself for the NMF model that you built earlier using the Wikipedia articles. Previously, you saw that the 3rd NMF feature value was high for the articles about actors Anne Hathaway and Denzel Washington. In this exercise, identify the topic of the corresponding NMF component.

The NMF model you built earlier is available as model, while words is a list of the words that label the columns of the word-frequency array.

After you are done, take a moment to recognise the topic that the articles about Anne Hathaway and Denzel Washington have in common!

Instructions
100 XP
Instructions
100 XP
Import pandas as pd.
Create a DataFrame components_df from model.components_, setting columns=words so that columns are labeled by the words.
Print components_df.shape to check the dimensions of the DataFrame.
Use the .iloc[] accessor on the DataFrame components_df to select row 3. Assign the result to component.
Call the .nlargest() method of component, and print the result. This gives the five words with the highest values for that component.



# Import pandas
import pandas as pd

# Create a DataFrame: components_df
components_df = pd.DataFrame(model.components_, columns=words)

# Print the shape of the DataFrame
print(components_df.shape)

# Select row 3: component
component = components_df.iloc[3]

# Print result of nlargest
print(component.nlargest())


<script.py> output:
    (6, 13125)
    film       0.627877
    award      0.253131
    starred    0.245284
    role       0.211451
    actress    0.186398
    Name: 3, dtype: float64
    
    
    
_________________________________________________________________________________________________________

Explore the LED digits dataset
In the following exercises, you'll use NMF to decompose grayscale images into their commonly occurring patterns. Firstly, explore the image dataset and see how it is encoded as an array. You are given 100 images as a 2D array samples, where each row represents a single 13x8 image. The images in your dataset are pictures of a LED digital display.

Instructions
100 XP
Import matplotlib.pyplot as plt.
Select row 0 of samples and assign the result to digit. For example, to select column 2 of an array a, you could use a[:,2]. Remember that since samples is a NumPy array, you can't use the .loc[] or iloc[] accessors to select specific rows or columns.
Print digit. This has been done for you. Notice that it is a 1D array of 0s and 1s.
Use the .reshape() method of digit to get a 2D array with shape (13, 8). Assign the result to bitmap.
Print bitmap, and notice that the 1s show the digit 7!
Use the plt.imshow() function to display bitmap as an image.

# Import pyplot
from matplotlib import pyplot as plt

# Select the 0th row: digit
digit = samples[0,:]

# Print digit
print(digit)

# Reshape digit to a 13x8 array: bitmap
bitmap = digit.reshape(13,8)

# Print bitmap
print(bitmap)

# Use plt.imshow to display bitmap
plt.imshow(bitmap, cmap='gray', interpolation='nearest')
plt.colorbar()
plt.show()



<script.py> output:
    [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  0.  0.  0.  0.
      0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.
      0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.
      0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.
      0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.
      0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
    [[ 0.  0.  0.  0.  0.  0.  0.  0.]
     [ 0.  0.  1.  1.  1.  1.  0.  0.]
     [ 0.  0.  0.  0.  0.  0.  1.  0.]
     [ 0.  0.  0.  0.  0.  0.  1.  0.]
     [ 0.  0.  0.  0.  0.  0.  1.  0.]
     [ 0.  0.  0.  0.  0.  0.  1.  0.]
     [ 0.  0.  0.  0.  0.  0.  0.  0.]
     [ 0.  0.  0.  0.  0.  0.  1.  0.]
     [ 0.  0.  0.  0.  0.  0.  1.  0.]
     [ 0.  0.  0.  0.  0.  0.  1.  0.]
     [ 0.  0.  0.  0.  0.  0.  1.  0.]
     [ 0.  0.  0.  0.  0.  0.  0.  0.]
     [ 0.  0.  0.  0.  0.  0.  0.  0.]]

_________________________________________________________________________________________________________


NMF learns the parts of images
Now use what you've learned about NMF to decompose the digits dataset. You are again given the digit images as a 2D array samples. This time, you are also provided with a function show_as_image() that displays the image encoded by any 1D array:

def show_as_image(sample):
    bitmap = sample.reshape((13, 8))
    plt.figure()
    plt.imshow(bitmap, cmap='gray', interpolation='nearest')
    plt.colorbar()
    plt.show()
After you are done, take a moment to look through the plots and notice how NMF has expressed the digit as a sum of the components!

Instructions
100 XP
Instructions
100 XP
Import NMF from sklearn.decomposition.
Create an NMF instance called model with 7 components. (7 is the number of cells in an LED display).
Apply the .fit_transform() method of model to samples. Assign the result to features.
To each component of the model (accessed via model.components_), apply the show_as_image() function to that component inside the loop.
Assign the row 0 of features to digit_features.
Print digit_features.

# Import NMF
from sklearn.decomposition import NMF

# Create an NMF model: model
model = NMF(n_components = 7)

# Apply fit_transform to samples: features
features = model.fit_transform(samples)

# Call show_as_image on each component
for component in model.components_:
    show_as_image(component)

# Assign the 0th row of features: digit_features
digit_features = features[0,:]

# Print digit_features
print(digit_features)

<script.py> output:
    [  4.76823559e-01   0.00000000e+00   0.00000000e+00   5.90605054e-01
       4.81559442e-01   0.00000000e+00   7.37557191e-16]
       
       

_________________________________________________________________________________________________________


PCA doesn't learn parts
Unlike NMF, PCA doesn't learn the parts of things. Its components do not correspond to topics (in the case of documents) or to parts of images, when trained on images. Verify this for yourself by inspecting the components of a PCA model fit to the dataset of LED digit images from the previous exercise. The images are available as a 2D array samples. Also available is a modified version of the show_as_image() function which colors a pixel red if the value is negative.

After submitting the answer, notice that the components of PCA do not represent meaningful parts of images of LED digits!

Instructions
100 XP
Import PCA from sklearn.decomposition.
Create a PCA instance called model with 7 components.
Apply the .fit_transform() method of model to samples. Assign the result to features.
To each component of the model (accessed via model.components_), apply the show_as_image() function to that component inside the loop.


# Import PCA
from sklearn.decomposition import PCA

# Create a PCA instance: model
model = PCA(n_components = 7)

# Apply fit_transform to samples: features
features = model.fit_transform(samples)

# Call show_as_image on each component
for component in model.components_:
    show_as_image(component)
    



_________________________________________________________________________________________________________


Building recommender
systems using NMF


Finding similar articles
● Engineer at a large online newspaper
● Task: recommend articles similar to article being read by
customer
● Similar articles should have similar topics



Strategy
● Apply NMF to the word-frequency array
● NMF feature values describe the topics
● ... so similar documents have similar NMF feature values
● Compare NMF feature values?


Apply NMF to the word-frequency array
In [1]: from sklearn.decomposition import NMF
In [2]: nmf = NMF(n_components=6)
In [3]: nmf_features = nmf.fit_transform(articles)
● articles is a word frequency array


Strategy
● Apply NMF to the word-frequency array
● NMF feature values describe the topics
● ... so similar documents have similar NMF feature values
● Compare NMF feature values?



Versions of articles
● Different versions of the same document have same topic proportions
● ... exact feature values may be different!
● E.g. because one version uses many meaningless words
● But all versions lie on the same line through the origin


Cosine similarity
● Uses the angle between the lines
● Higher values means more similar
● Maximum value is 1, when angle is 0˚


Calculating the cosine similarities
In [4]: from sklearn.preprocessing import normalize
In [5]: norm_features = normalize(nmf_features)
In [6]: current_article = norm_features[23,:] # if has index 23
In [7]: similarities = norm_features.dot(current_article)
In [8]: print(similarities)
[ 0.7150569 0.26349967 0.40210445 ..., 0.70462768 0.20323616
 0.05047817]
 
 
 
 DataFrames and labels
In [9]: import pandas as pd
In [10]: norm_features = normalize(nmf_features)
In [11]: df = pd.DataFrame(norm_features, index=titles)
In [12]: current_article = df.loc['Dog bites man']
In [13]: similarities = df.dot(current_article)
● Label similarities with the article titles, using a DataFrame
● Titles given as a list: titles



DataFrames and labels
In [14]: print(similarities.nlargest())
Dog bites man 1.000000
Hound mauls cat 0.979946
Pets go wild! 0.979708
Dachshunds are dangerous 0.949641
Our streets are no longer safe 0.900474
dtype: float64


_________________________________________________________________________________________________________

Which articles are similar to 'Cristiano Ronaldo'?
In the video, you learned how to use NMF features and the cosine similarity to find similar articles. Apply this to your NMF model for popular Wikipedia articles, by finding the articles most similar to the article about the footballer Cristiano Ronaldo. The NMF features you obtained earlier are available as nmf_features, while titles is a list of the article titles.

Instructions
100 XP
Import normalize from sklearn.preprocessing.
Apply the normalize() function to nmf_features. Store the result as norm_features.
Create a DataFrame df from norm_features, using titles as an index.
Use the .loc[] accessor of df to select the row of 'Cristiano Ronaldo'. Assign the result to article.
Apply the .dot() method of df to article to calculate the cosine similarity of every row with article.
Print the result of the .nlargest() method of similarities to display the most similiar articles. This has been done for you, so hit 'Submit Answer' to see the result!


# Perform the necessary imports
import pandas as pd
from sklearn.preprocessing import normalize

# Normalize the NMF features: norm_features
norm_features = normalize(nmf_features)

# Create a DataFrame: df
df = pd.DataFrame(norm_features, index=titles)

# Select the row corresponding to 'Cristiano Ronaldo': article
article = df.loc['Cristiano Ronaldo']

# Compute the dot products: similarities
similarities = df.dot(article)

# Display those with the largest cosine similarity
print(similarities.nlargest())

_________________________________________________________________________________________________________

Recommend musical artists part I
In this exercise and the next, you'll use what you've learned about NMF to recommend popular music artists! You are given a sparse array artists whose rows correspond to artists and whose column correspond to users. The entries give the number of times each artist was listened to by each user.

In this exercise, build a pipeline and transform the array into normalized NMF features. The first step in the pipeline, MaxAbsScaler, transforms the data so that all users have the same influence on the model, regardless of how many different artists they've listened to. In the next exercise, you'll use the resulting normalized NMF features for recommendation!

This data is part of a larger dataset available here.

Instructions
100 XP
Instructions
100 XP
Import:
NMF from sklearn.decomposition.
Normalizer and MaxAbsScaler from sklearn.preprocessing.
make_pipeline from sklearn.pipeline.
Create an instance of MaxAbsScaler called scaler.
Create an NMF instance with 20 components called nmf.
Create an instance of Normalizer called normalizer.
Create a pipeline called pipeline that chains together scaler, nmf, and normalizer.
Apply the .fit_transform() method of pipeline to artists. Assign the result to norm_features.



 # Perform the necessary imports
from sklearn.decomposition import NMF
from sklearn.preprocessing import Normalizer, MaxAbsScaler
from sklearn.pipeline import make_pipeline

# Create a MaxAbsScaler: scaler
scaler = MaxAbsScaler()

# Create an NMF model: nmf
nmf = NMF(n_components=20)

# Create a Normalizer: normalizer
normalizer = Normalizer()

# Create a pipeline: pipeline
pipeline = make_pipeline(scaler, nmf, normalizer)

# Apply fit_transform to artists: norm_features
norm_features = pipeline.fit_transform(artists)


_________________________________________________________________________________________________________

Recommend musical artists part II
Suppose you were a big fan of Bruce Springsteen - which other musicial artists might you like? Use your NMF features from the previous exercise and the cosine similarity to find similar musical artists. A solution to the previous exercise has been run, so norm_features is an array containing the normalized NMF features as rows. The names of the musical artists are available as the list artist_names.

Instructions
100 XP
Import pandas as pd.
Create a DataFrame df from norm_features, using artist_names as an index.
Use the .loc[] accessor of df to select the row of 'Bruce Springsteen'. Assign the result to artist.
Apply the .dot() method of df to artist to calculate the dot product of every row with artist. Save the result as similarities.
Print the result of the .nlargest() method of similarities to display the artists most similar to 'Bruce Springsteen'.


# Import pandas
import pandas as pd

# Create a DataFrame: df
df = pd.DataFrame(norm_features,index=artist_names)

# Select row of 'Bruce Springsteen': artist
artist = df.loc['Bruce Springsteen']

# Compute cosine similarities: similarities
similarities = df.dot(artist)

# Display those with highest cosine similarity
print(similarities.nlargest())


<script.py> output:
    Bruce Springsteen    1.000000
    Neil Young           0.957535
    Van Morrison         0.876509
    Leonard Cohen        0.866697
    Bob Dylan            0.864827
    dtype: float64







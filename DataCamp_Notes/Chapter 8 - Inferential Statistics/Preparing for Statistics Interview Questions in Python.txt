Preparing for Statistics Interview Questions in Python

Course Description
Are you looking to land that next job or hone your statistics interview skills to stay sharp? Get ready to master classic interview concepts ranging from conditional probabilities to A/B testing to the bias-variance tradeoff, and much more! You’ll work with a diverse collection of datasets including web-based experiment results and Australian weather data. Following the course, you’ll be able to confidently walk into your next interview and tackle any statistics questions with the help of Python!

=======================================================================================================================

1
Probability and Sampling Distributions
FREE
0%
This chapter kicks the course off by reviewing conditional probabilities, Bayes' theorem, and central limit theorem. Along the way, you will learn how to handle questions that work with commonly referenced probability distributions.

__________________________________________________________________________________________________________________________________

Bayes' theorem applied
Let's actually solve out a pretty straightforward, yet typical Bayes' theorem interview problem. You have two coins in your hand. Out of the two coins, one is a real coin and the other one is a faulty coin with tails on both sides.

You are blindfolded and forced to choose a random coin and then toss it in the air. The coin lands with tails facing upwards. Find the probability that this is the faulty coin.

Instructions
0 XP
Print the probability of the coin landing tails.
Print the probability of the coin being faulty.
Print the probability of the coin being faulty and landing tails.
Print and solve for the probability that the coin is faulty, given it came down on tails.


# Print P(tails)
print(3 / 4)

# Print P(faulty)
print(1 / 2)

# Print P(tails and faulty)
print(0.5 * 1)

# Print P(faulty | tails)
print(0.5 / 0.75)


All done! Congrats on completing the first lesson and moving a step closer to mastering the statistics interview in python! Keep practicing Bayes' theorem and reviewing different types of probability questions that might get thrown your way. Let's move on and talk about another popular interview topic: central limit theorem.

________________________________________________________________________________________________________________________-

Samples from a rolled die
Let's work through generating a simulation using the numpy package. You'll work with the same scenario from the slides, simulating rolls from a standard die numbered 1 through 6, using the randint() function. Take a look at the documentation for this function if you haven't encountered it before.

Starting with a small sample and working your way up to a larger sample, examine the outcome means and come to a conclusion about the underlying theorem.

Instructions
100 XP
Generate a sample of 10 die rolls using the randint() function; assign it to our small variable.
Assign the mean of the sample to small_mean and print the results; notice how close it is to the true mean.
Similarly, create a larger sample of 1000 die rolls and assign the list to our large variable.
Assign the mean of the larger sample to large_mean and print the mean; which theorem is at work here?

from numpy.random import randint

# Create a sample of 10 die rolls
small = randint(1, 7, 10)

# Calculate and print the mean of the sample
small_mean = small.mean()
print(small_mean)

# Create a sample of 1000 die rolls
large = randint(1, 7, 1000)

# Calculate and print the mean of the large sample
large_mean = large.mean()
print(large_mean)

<script.py> output:
    3.4
    3.486
    
    
    Good job! Notice how the mean of the large sample has gotten closer to the true expected mean value of 3.5 for a rolled die. Which theorem did you say was being demonstrated here? Was it the law of large numbers? If so, you're correct! It's important to distinguish between the law of large numbers and central limit theorem in interviews.
________________________________________________________________________________________________________________________
Simulating central limit theorem
Now that we have some practice creating a sample, we'll look at simulating the central limit theorem, similar to what you saw in the slides. We'll also continue dealing with a standard die numbered 1 through 6.

In order to do this, you'll take a collection of sample means from numpy and examine the distribution of them using the matplotlib package, which has been imported as plt for the rest of the chapter.

Instructions 1/3
35 XP
1
Create a list named means with 1000 sample means from samples of 30 rolled dice by using list comprehension.

2
Create and show a histogram of the means using the hist() function; examine the shape of the distribution.

3
Adapt your code to visualize only 100 samples in the means list; did the distribution change at all?



from numpy.random import randint

# Create a list of 1000 sample means of size 30
means = [randint(1, 7, 30).mean() for i in range(1000)]

from numpy.random import randint

# Create a list of 1000 sample means of size 30
means = [randint(1, 7, 30).mean() for i in range(1000)]

# Create and show a histogram of the means
plt.hist(means)
plt.show()

from numpy.random import randint

# Adapt code for 100 samples of size 30
means = [randint(1, 7, 30).mean() for i in range(100)]

# Create and show a histogram of the means
plt.hist(means)
plt.show()


Nice! Note how whether we took 100 or 1000 sample means, the distribution was still approximately normal. This will always be the case when we have a large enough sample (typically above 30). That's the central limit theorem at work. Remember why it's so important. It serves as the basis for all statistical experiments that you'll do!

________________________________________________________________________________________________________________________

Bernoulli distribution
Let's start simple with the Bernoulli distribution. In this exercise, you'll generate sample data for a Bernoulli event and then examine the visualization produced. Before we start, make yourself familiar with the rvs() function within scipy.stats that we'll use for sampling over the next few exercises.

Let's stick to the prior example of flipping a fair coin and checking the outcome: heads or tails. Remember that matplotlib is already imported as plt for you.

Instructions 1/3
35 XP
1
Generate a sample using the rvs() function with size set to 100; assign it to the data variable.

2
Create and display a histogram using the hist() function; examine the shape of the distribution.

3
Adapt the code to take a sample of 1000 observations this time.


# Generate bernoulli data
from scipy.stats import bernoulli
data = bernoulli.rvs(p=0.5, size=100)

# Generate bernoulli data
from scipy.stats import bernoulli
data = bernoulli.rvs(p=0.5, size=100)

# Plot distribution
plt.hist(data)
plt.show()

# Generate bernoulli data
from scipy.stats import bernoulli
data = bernoulli.rvs(p=0.5, size=1000)

# Plot distribution
plt.hist(data)
plt.show()

Good job! Notice that heads and tails didn't have the exact same probability with a sample size of just 100. This is no fluke — when sampling, we won't always get perfect results. We can increase our accuracy however, as you saw when you upped the sample size to 1,000 observations. Now let's move forward to some more interesting distributions!
________________________________________________________________________________________________________________________

Binomial distribution
As we touched on in the slides, the binomial distribution is used to model the number of successful outcomes in trials where there is some consistent probability of success.

For this exercise, consider a game where you are trying to make a ball in a basket. You are given 10 shots and you know that you have an 80% chance of making a given shot. To simplify things, assume each shot is an independent event.

Instructions
100 XP
Generate some data for the distribution using the rvs() function with size set to 1000; assign it to the data variable.
Display a matplotlib histogram; examine the shape of the distribution.
Assign the probability of making 8 or less shots to prob1 and print the result.
Assign the probability of making all 10 shots to prob2 and print the result.

# Generate binomial data
from scipy.stats import binom
data = binom.rvs(n=10, p=0.8, size=1000)

# Plot the distribution
plt.hist(data)
plt.show()

# Assign and print probability of 8 or less successes
prob1 = binom.cdf(k=8, n=10, p=0.8)
print(prob1)

# Assign and print probability of all 10 successes
prob2 = binom.pmf(k=10, n=10, p=0.8)
print(prob2)


<script.py> output:
    0.6241903616
    0.10737418240000005

Nice job! Notice that we started out simple by just showing the general shape of the distribution, but quickly moved on to actual application. Remember, interviewers like to start out with fundamental concepts before getting incrementally more complex. By mastering the basics, you put yourself in a much better position right off the bat!

________________________________________________________________________________________________________________________

Normal distribution
On to the most recognizable and useful distribution of the bunch: the normal or Gaussian distribution. In the slides, we briefly touched on the bell-curve shape and how the normal distribution along with the central limit theorem enables us to perform hypothesis tests.

Similar to the previous exercises, here you'll start by simulating some data and examining the distribution, then dive a little deeper and examine the probability of certain observations taking place.

Instructions
100 XP
Generate the data for the distribution by using the rvs() function with size set to 1000; assign it to the data variable.
Display a matplotlib histogram; examine the shape of the distribution.
Given a standardized normal distribution, what is the probability of an observation greater than 2?
Looking at our sample, what is the probability of an observation greater than 2?


# Generate normal data
from scipy.stats import norm
data = norm.rvs(size=1000)

# Plot distribution
plt.hist(data)
plt.show()

# Compute and print true probability for greater than 2
true_prob = 1 - norm.cdf(2)
print(true_prob)

# Compute and print sample probability for greater than 2
sample_prob = sum(obs > 2 for obs in data) / len(data)
print(sample_prob)

<script.py> output:
    0.02275013194817921
    0.014
    
All done! How close is the result from the true distribution vs. our sample distribution? Do these results make since in the context of the 68-95-99.7 rule discussed in the slides? Make sure to keep reviewing and going deeper on the topic of the normal distribution, as it's a favorite among interviewers. Congrats on finishing the first chapter and moving closer to acing your next interview!    
    
   

=======================================================================================================================
2
Exploratory Data Analysis
0%
In this chapter, you will prepare for statistical concepts related to exploratory data analysis. The topics include descriptive statistics, dealing with categorical variables, and relationships between variables. The exercises will prepare you for an analytical assessment or stats-based coding question.

________________________________________________________________________________________________________________________

Mean or median
As data scientists, we often look to describe data as concisely as possible. This brings us to the two most common measures of centrality: mean and median. In this exercise, you'll examine a couple different scenarios and decide which metric is optimal for effectively describing the data.

More concretely, you'll be exploring Australian weather data containing features related to temperature and wind speeds. This dataset has already been imported as weather and both the matplotlib and pandas packages have been imported as plt and pd for you to use the rest of the chapter as well.

Instructions 1/3
35 XP
1
Plot the distribution of the Temp3pm column using the hist() function; is the data skewed at all?

2
Assign and print the mean and median for the Temp3pm column; which do you think is a better representation of the data?

3
Adapt the code to explore a different column; see how the results for Temp9am look.


# Visualize the distribution 
plt.hist(weather.Temp3pm)
plt.show()


# Visualize the distribution 
plt.hist(weather['Temp3pm'])
plt.show()

# Assign the mean to the variable and print the result
mean = weather.Temp3pm.mean()
print('Mean:', mean)

# Assign the median to the variable and print the result
median = weather.Temp3pm.median()
print('Median:', median)


# Visualize the distribution 
plt.hist(weather['Temp9am'])
plt.show()

# Assign the mean to the variable and print the result
mean = weather['Temp9am'].mean()
print('Mean:', mean)

# Assign the median to the variable and print the result
median = weather['Temp9am'].median()
print('Median:', median)


Great work! If you look closely here, you can see the that distribution for Temp3pm was skewed left while the distribution for Temp9am was more skewed right. For this reason, the median was higher than the mean initially, but after adapting our code we see that the mean became higher than the median due to the change in skewness. While neither column is harshly skewed, it is enough that we should consider using median instead of mean due to the robustness of the metric.

________________________________________________________________________________________________________________________

Standard deviation by hand
In the video, we talked about measures of variability, and discussed standard deviation as the measure that is used most commonly. It's pretty important that you have a grasp on this concept, as interviewers will likely hit on it early on in the process through a coding assignment or something more conceptual.

Here, you'll simulate this experience by computing standard deviation by hand, meaning that you won't use any existing functions like std() to get your results.

Instructions
100 XP
Without using the mean() function, compute the mean of our nums list defined for you.
Use the computed variance value along with the math.sqrt() function to get the standard deviation; print your result.
Check your work by printing the actual standard deviation with the np.std() function mentioned earlier.

# Create a sample list
import math
nums = [1, 2, 3, 4, 5]

# Compute the mean of the list
mean = sum(nums) / len(nums)

# Compute the variance and print the std of the list
variance = sum(pow(x - mean, 2) for x in nums) / len(nums)
std = math.sqrt(variance)
print(std)

# Compute and print the actual result from numpy
real_std = np.array(nums).std()
print(real_std)

Good job! Your output should match the results from the std() function. If you can do this, you probably have a solid grasp on the standard deviation formula and how it's calculated. Interviewers like this question especially because it gives them an opportunity to gut-check your coding skills while also getting a feel for your stats background.
________________________________________________________________________________________________________________________

Encoding techniques
In the slides, we discussed two encoding techniques: label encoding and one-hot encoding. In practice, the technique that you use is determined by the situation at hand. That being said, you should have both of these at your disposal for your interview.

In this exercise, you'll practice implementing both of these techniques on the same dataset of laptop prices that you saw earlier, pre-loaded within the laptops variable.

Instructions 1/2
50 XP
1
Use the created label encoder object in encoder to transform the Company column; print the results.
One-hot encode laptops2 copied from our original DataFrame on the Company column; print the head of the DataFrame.


from sklearn import preprocessing

# Create the encoder and print our encoded new_vals
encoder = preprocessing.LabelEncoder()
new_vals = encoder.fit_transform(laptops.Company)
print(new_vals)


# One-hot encode Company for laptops2
laptops2 = pd.get_dummies(data=laptops2, columns=['Company'])
print(laptops2.head())


Congrats! You can now encode categorical variables using the power of the pandas and scikit-learn packages. With more practice, you'll get a better feel for which scenarios demand each technique. Note that one-hot encoding can create extremely highly-dimensional data if you aren't careful.

________________________________________________________________________________________________________________________
Exploring laptop prices
We walked through an example in the video of surface-level data analysis of categorical variables. Here, you'll perform a similar task first-hand. Taken from the same pre-loaded laptops dataset, you'll work with three separate brands: Acer, Asus, and Toshiba.

You'll produce some initial information about the dataset, create a countplot of the companies, and analyze the relationship of each against the price in euros.

All of the usual packages have been imported for you. We've also gone ahead and imported the seaborn package as sns for you, which we'll use for visualization.

Instructions 1/3
35 XP
Get some initial information about the data using the info() function; here you can see the null values and data types quite easily.

Visualize how many observations are from each brand by using the countplot() function on the Company column; examine the results.

Plot the relationship between the Price and Company columns; what can you conclude from this?

# Get some initial info about the data
laptops.info()


# Get some initial info about the data
laptops.info()

# Produce a countplot of companies
sns.countplot(laptops.Company)
plt.show()


# Visualize the relationship with price
laptops.boxplot('Price', 'Company', rot=30)
plt.show()

What did you conclude here? It appears that Asus is the most common brand while Toshiba is less common. Furthermore, despite a few outliers, there is a steady increase in price as we move from Acer to Asus to Toshiba models. During your interview prep, don't forget to emphasize communicating results. Companies are looking for someone that can break down insights and share them effectively with non-technical team members. Recording yourself is an excellent way to practice this.
________________________________________________________________________________________________________________________


Types of relationships
How do we effectively identify these relationships in practice? The first and often only step you need, is to visualize the data using a scatter plot. In this exercise, you'll examine a few different relationships, produce the scatter plot, and then consider what each plot tells us about the relationship.

You'll work with a few different features in the Australian weather dataset from before, imported as weather for you to use. For the sake of this exercise, make sure you pass features in the order they are given to you!

Instructions 1/3
35 XP
1
Visualize the relationship between the MinTemp and MaxTemp variables using the scatter() function.
Adapt your code to visualize the relationship between the MaxTemp and Humidity9am variables instead.
Lastly, adapt your code once again for MinTemp and Humidity3pm variables.


# Display a scatter plot and examine the relationship
plt.scatter(weather.MinTemp, weather.MaxTemp)
plt.show()


# Display a scatter plot and examine the relationship
plt.scatter(weather.MaxTemp, weather.Humidity9am)
plt.show()

# Display a scatter plot and examine the relationship
plt.scatter(weather.MinTemp, weather.Humidity3pm)
plt.show()


Good job! What did you conclude for each example? The first example gave us a decently strong positive relationship, the second example was about as strong but negative, and the final example had no apparent relationship. Understanding all of these terms is important, but make sure you understand what they mean in practice. In our first example, we saw that minimum temperature gave us some information about what the maximum temperature will be later on.
________________________________________________________________________________________________________________________

Pearson correlation
You know how to identify different relationships based on scatter plots, but let's take a more practical approach now and dive a bit deeper into one relationship. We'll stick to the same version of the weather dataset that we just used.

You'll analyze a seaborn pair plot of some features before drilling down for further analysis and coming to a conclusion about the correlation. This process should be something that you feel quite comfortable with.

Instructions 1/4
25 XP
1
2
3
4
Display a seaborn pair plot for the weather dataset; explore the output a bit once it loads.
Take a closer look at the relationship between the Humidity9am and Humidity3pm variables using the scatter() function.
Calculate the Pearson correlation coefficient between the Humidity9am and Humidity3pm variables using pandas built-in corr() function; would you say these features are correlated?
Assign the squared result to the r2 variable and print it out below; what does this means exactly?

# Generate the pair plot for the weather dataset
sns.pairplot(weather)
plt.show()

# Look at the scatter plot for the humidity variables
plt.scatter(weather.Humidity9am, weather.Humidity3pm)
plt.show()

# Compute and print the Pearson correlation
r = weather['Humidity9am'].corr(weather['Humidity3pm'])
print(r)


# Compute and print the Pearson correlation
r = weather['Humidity9am'].corr(weather['Humidity3pm'])

# Calculate the r-squared value and print the result
r2 = r**2
print(r2)

________________________________________________________________________________________________________________________

Sensitivity to outliers
When we are analyzing the relationship of more than one variable, correlation is a great start. But how does correlation hold up against some more interesting datasets? How well does it hold up against outliers?

In this exercise, you will plot and compute the correlation for a dataset with an outlier and then remove it and see what changes. In the end, you want to see how correlation performs and come to a conclusion about when and where you should use it.

A sample dataset from the famous Anscombe's quartet has been imported for you as the df variable, along with the all the packages used previously in this chapter.

Instructions 1/3
35 XP
1
2
3
Display a matplotlib scatter plot of the X and Y features; notice the outlier in the top right.
Compute and print the Pearson correlation for the X and Y columns using the built-in corr() function within the pandas package.
Drop the outlier at index 2 from our dataset using the drop() function; assign and print the correlation again.

# Display the scatter plot of X and Y
plt.scatter(df.X, df.Y)
plt.show()

# Compute and print the correlation
corr  = df['X'].corr(df['Y'])
print(corr)

# Drop the outlier from the dataset
df = df.drop(2)

# Compute and print the correlation once more
new_corr  = df['X'].corr(df['Y'])
print(new_corr)

Great! Notice how our correlation initially suffered due to the outlier, but became nearly perfect once it was removed. This effectively conveyed the biggest complaint about Pearson correlation; the metric is not very robust to outliers, so data scientists often must seek other tools when the outliers can't or shouldn't be easily removed. It's a good idea to think about the pros and cons of techniques to stand out during your interview!



=======================================================================================================================

3
Statistical Experiments and Significance Testing
0%
Prepare to dive deeper into crucial concepts regarding experiments and testing by reviewing confidence intervals, hypothesis testing, multiple tests, and the role that power and sample size play. We'll also discuss types of errors, and what they mean in practice.

________________________________________________________________________________________________________________________

Confidence interval by hand
There are two common ways that interviewers will touch on confidence intervals; they will either ask you to explain it in simple terms, or elaborate on how they are calculated, possibly having you implement one. In this exercise, you'll practice the latter by producing a confidence interval by hand, using no packages other than those imported for you.

We have gone ahead and assigned the appropriate z-score for a 95% confidence interval and sample mean to the z_score and sample_mean variables to simplify things a bit.

Instructions
100 XP
Compute the standard error and the margin of error using the sem() function and z_score variable imported for you.
Compute and print the lower boundary of our confidence interval using the sample_mean variable imported for you.
Compute and print the upper boundary of our confidence interval using the sample_mean variable imported for you.

from scipy.stats import sem, t
data = [1, 2, 3, 4, 5]
confidence = 0.95

# Compute the standard error and margin of error
std_err = sem(data)
margin_error = std_err * z_score

# Compute and print the lower threshold
lower = sample_mean - margin_error
print(lower)

# Compute and print the upper threshold
upper = sample_mean + margin_error
print(upper)

Good work showing your understanding of how confidence intervals are calculated! Now you'll be prepared if interviewers ever throw this at you in a coding assessment. Don't forget about the conceptual understanding, though. Make sure you're comfortable explaining confidence intervals to a beginner in an intuitive way.

________________________________________________________________________________________________________________________
Applying confidence intervals
In practice, you aren't going to hand-code confidence intervals. Let's utilize the statsmodels package to streamline this process and examine some more tendencies of interval estimates.

In this exercise, we've generated a binomial sample of the number of heads in 50 fair coin flips saved as the heads variable. You'll compute a few different confidence intervals for this sample, and then scale your work for 10 similar samples.

The proportion_confint() function has already been imported to help you compute confidence intervals.

Instructions 1/3
35 XP
1
Compute and print a 99% confidence interval for 50 trials; does it contain the true proportion of a fair coin flip?
Adapt your code to generate a 90% confidence interval this time; does it contain the true proportion this time?


Examine your confidence interval results from the last step. You might see at least one confidence interval that does not contain 0.5, the true population proportion for a fair coin flip. You could decrease the likelihood of this happening by increasing your confidence level or lowering the alpha value.
Repeat this process of sampling, computing the confidence interval, and printing the result 10 times using a loop.



# Compute and print the 99% confidence interval
confidence_int = proportion_confint(heads, 50, 0.01)
print(confidence_int)

# Compute and print the 90% confidence interval
confidence_int = proportion_confint(heads, 50, 0.1)
print(confidence_int)

# Repeat this process 10 times 
heads = binom.rvs(50, 0.5, size=10)
for val in heads:
    confidence_interval = proportion_confint(val, 50, .10)
    print(confidence_interval)




<script.py> output:
    (0.35844514241179504, 0.721554857588205)

<script.py> output:
    (0.42406406993539053, 0.6559359300646095)

<script.py> output:
    (0.3440640699353905, 0.5759359300646095)
    (0.3245317440082245, 0.5554682559917755)
    (0.3836912846323326, 0.6163087153676674)
    (0.42406406993539053, 0.6559359300646095)
    (0.36378436885322046, 0.5962156311467796)
    (0.5283436332470393, 0.7516563667529608)
    (0.42406406993539053, 0.6559359300646095)
    (0.3836912846323326, 0.6163087153676674)
    (0.36378436885322046, 0.5962156311467796)
    (0.36378436885322046, 0.5962156311467796)
    
________________________________________________________________________________________________________________________


  One tailed z-test
We know now that hypothesis tests can come in several forms. In this exercise, you'll implement a one tailed z-test on test data from tracking conversion on a mobile app. The data has been imported as results and numpy has already been imported for you along with pandas as well.

The treatment group represents some graphic alteration that we expect to improve the conversion rate of users. Run a test with alpha as .05 and find out if the change actually helped.

Instructions 1/4
25 XP
1
2
3
4
Assign and print the mean conversion rate for each group using the groupby() function on the Group column.
Assign the number of control conversions to num_control and the total number of trials to the total_control variable by slicing the DataFrame.
Similarly, assign the same values for the treatment group by slicing the DataFrame.
Run the z-test using the proportions_ztest() function and passing it the count and nobs variables.    

# Assign and print the conversion rate for each group
conv_rates = results.groupby('Group').mean()
print(conv_rates)

# Assign the number of control conversions and trials
num_control = results[results['Group'] == 'control']['Converted'].sum()
total_control = len(results[results['Group'] == 'control'])

# Assign the number of conversions and total trials
num_control = results[results['Group'] == 'control']['Converted'].sum()
total_control = len(results[results['Group'] == 'control'])

# Assign the number of conversions and total trials
num_treat = results[results['Group']=='treatment']['Converted'].sum()
total_treat = len(results[results['Group']=='treatment'])


# Assign the number of conversions and total trials
num_control = results[results['Group'] == 'control']['Converted'].sum()
total_control = len(results[results['Group'] == 'control'])

# Assign the number of conversions and total trials
num_treat = results[results['Group'] == 'treatment']['Converted'].sum()
total_treat = len(results[results['Group'] == 'treatment'])

from statsmodels.stats.proportion import proportions_ztest
count = np.array([num_treat, num_control]) 
nobs = np.array([total_treat, total_control])

# Run the z-test and print the result 
stat, pval = proportions_ztest(count, nobs, alternative="larger")
print('{0:0.3f}'.format(pval))


Good job! You see that our test gave us a resulting p-value of .009 which falls under our alpha value of .05, so we can conclude that there is an effect and, therefore, we reject the null hypothesis. It looks like the change actually did have a noticeable positive effect on conversion rate!

________________________________________________________________________________________________________________________

Two tailed t-test
In this exercise, you'll tackle another type of hypothesis test with the two tailed t-test for means. More concretely, you'll run the test on our laptops dataset from before and try to identify a significant difference in price between Asus and Toshiba.

Once again, we've imported all of the standard packages. Once you get your result, don't forget to make an actionable conclusion.

Instructions 1/3
35 XP
1
2
3
Assign and print the mean price for each group using the groupby() function on the Company feature.
Assign the prices of each group to their respective variable.
Run the t-test and print the results using the imported ttest_ind() function; what's your conclusion?


# Display the mean price for each group
prices = laptops.groupby('Company').mean()
print(prices)

# Assign the prices of each group
asus = laptops[laptops['Company'] == 'Asus']['Price']
toshiba = laptops[laptops['Company'] == 'Toshiba']['Price']

# Assign the prices of each group
asus = laptops[laptops['Company'] == 'Asus']['Price']
toshiba = laptops[laptops['Company'] == 'Toshiba']['Price']

# Run the t-test
from scipy.stats import ttest_ind
tstat, pval = ttest_ind(asus, toshiba)
print('{0:0.3f}'.format(pval))

With a p-value of .133, we cannot reject the null hypothesis! There's not enough evidence here to conclude that Toshiba laptops are significantly more expensive than Asus. With that being said, .133 is fairly close to reasonable significance so we may want to run another test or examine this further. Interviewers won't hesitate to throw you tricky situations like this to see how you handle them. Come to a conclusion and make a strong argument for it. Good job!

________________________________________________________________________________________________________________________

Calculating sample size
Let's finish up our dive into statistical tests by performing power analysis to generate needed sample size. Power analysis involves four moving parts:

Sample size
Effect size
Minimum effect
Power
In this exercise, you're working with a website and want to test for a difference in conversion rate. Before you begin the experiment, you must decide how many samples you'll need per variant using 5% significance and 95% power.

Instructions 1/3
35 XP
1
Standardize the effect of a conversion rate increase from 20% to 25% success using the proportion_effectsize() function.
Calculate and print the needed sample size using the zt_ind_solve_power() function.

Adapt your code to solve for needed sample size using 80% power instead; observe what happens.

# Standardize the effect size
from statsmodels.stats.proportion import proportion_effectsize
std_effect = proportion_effectsize(0.2,0.25)


# Standardize the effect size
from statsmodels.stats.proportion import proportion_effectsize
std_effect = proportion_effectsize(.20, .25)

# Assign and print the needed sample size
from statsmodels.stats.power import  zt_ind_solve_power
sample_size = zt_ind_solve_power(effect_size=std_effect, nobs1=None, alpha=0.05, power=0.95)
print(sample_size)


# Standardize the effect size
from statsmodels.stats.proportion import proportion_effectsize
std_effect = proportion_effectsize(.20, .25)

# Assign and print the needed sample size
from statsmodels.stats.power import  zt_ind_solve_power
sample_size = zt_ind_solve_power(effect_size=std_effect, nobs1=None, alpha=.05, power=0.8)
print(sample_size)

All done! Notice how lowering the power allowed you fewer observations in your sample, yet increased your chance of a Type II error. Remember that doing these calculations by hand is quite difficult, so you may be asked to show or explain these tradeoffs with whiteboarding rather than programming.



________________________________________________________________________________________________________________________


Visualizing the relationship
Now that we've gone over the effect on certain errors and calculated the necessary sample size for different power values, let's take a step back and look at the relationship between power and sample size with a useful plot.

In this exercise, we'll switch gears and look at a t-test rather than a z-test. In order to visualize this, use the plot_power() function that shows sample size on the x-axis with power on the y-axis and different lines representing different minimum effect sizes.

Instructions
100 XP
Assign a TTestIndPower() object to the results variable.
Visualize the relationship between power and sample size using the plot_power() function with the appropriate parameter values; what do you notice?

Good work! Notice that not only does an increase in power result in a larger sample size, but this increase grows exponentially as the minimum effect size is increased. Once again, power analysis can get confusing with all of these interconnected moving parts, but by mastering the relationships at hand, you'll be ready when the question comes up from your interviewer!

________________________________________________________________________________________________________________________

Calculating error rates
We talked a bit about the multiple comparisons problem in the slides, but let's take things a step further. In this exercise, you'll look into how the phenomenon affects error rate more precisely.

Your colleague is strongly considering running 60 distinct hypothesis tests. In order to convince them otherwise, compute the probability of a Type I error for 60 hypothesis tests with a single-test 5% significance level.

Compute and print the probability of your colleague getting a Type I error.
You successfully talked them down to 30 tests; adapt your code to compute and print the new error rate.
One last try; adapt your code to compute and print the error rate for 10 tests.


# Print error rate for 60 tests with 5% significance
error_rate = 1 - (.95**(60))
print(error_rate)

# Print error rate for 30 tests with 5% significance
error_rate = 1 - (.95**(30))
print(error_rate)

# Print error rate for 10 tests with 5% significance
error_rate = 1 - (.95**(10))
print(error_rate)



<script.py> output:
    0.953930201013048

<script.py> output:
    0.7853612360570628

<script.py> output:
    0.4012630607616213

As you can see, the probability of encountering an error is still extremely high. This is where the Bonferroni correction comes in. While a bit conservative, it controls the family-wise error rate for circumstances like these to avoid the high probability of a Type I error. We'll go over this specific method in the next exercise!

________________________________________________________________________________________________________________________
Bonferroni correction
Let's implement multiple hypothesis tests using the Bonferroni correction approach that we discussed in the slides. You'll use the imported multipletests() function in order to achieve this.

Use a single-test significance level of .05 and observe how the Bonferroni correction affects our sample list of p-values already created.

Instructions
100 XP
Compute a list of the Bonferroni adjusted p-values using the imported multipletests() function.
Print the results of the multiple hypothesis tests returned in index 0 of your p_adjusted variable.
Print the p-values themselves returned in index 1 of your p_adjusted variable.

from statsmodels.sandbox.stats.multicomp import multipletests
pvals = [.01, .05, .10, .50, .99]

# Create a list of the adjusted p-values
p_adjusted = multipletests(pvals, alpha=0.05, method='bonferroni')

# Print the resulting conclusions
print(p_adjusted[0])

# Print the adjusted p-values themselves 
print(p_adjusted[1])


<script.py> output:
    [ True False False False False]
    [0.05 0.25 0.5  1.   1.  ]


Good work! As you can see, the Bonferroni correction did it's job and corrected the family-wise error rate for our 5 hypothesis test results. In the end, only one of the tests remained signficant. If you're interested, check out some of the other methods, but interviewers typically won't get too far in the weeds here.

=======================================================================================================================

4
Regression and Classification
0%
Wrapping up, we'll address concepts related closely to regression and classification models. The chapter begins by reviewing fundamental machine learning algorithms and quickly ramps up to model evaluation, dealing with special cases, and the bias-variance tradeoff.


________________________________________________________________________________________________________________________


Linear regression
In this exercise, you'll implement a simple linear regression model. Get ready to make predictions, visualize the model fit, and analyze the formula used to generate your fit.

By now, you're probably comfortable with the weather dataset that we'll be using. Your dependent variable will be the Humidity3pm feature. All of the standard packages have been imported for you.

Instructions 1/4
25 XP
1
2
3
4
Assign a LinearRegression() object to the lm variable; fit your model on the next line.
Assign the predictions from your model to preds using the predict() function; print the list once you have it.
Visualize the relationship between X and y with the scatter() function, then plot your predictions on top using the plot() function.
Assign and print the coefficient for your independent variable; what does this mean?



from sklearn.linear_model import LinearRegression 
X = np.array(weather['Humidity9am']).reshape(-1,1)
y = weather['Humidity3pm']

# Create and fit your linear regression model
lm = LinearRegression()
lm.fit(X, y)


from sklearn.linear_model import LinearRegression 
X = np.array(weather['Humidity9am']).reshape(-1,1)
y = weather['Humidity3pm']

# Create and fit your linear regression model
lm = LinearRegression()
lm.fit(X, y)

# Assign and print predictions
preds = lm.predict(X)
print(preds)


from sklearn.linear_model import LinearRegression 
X = np.array(weather['Humidity9am']).reshape(-1,1)
y = weather['Humidity3pm']

# Create and fit your linear regression model
lm = LinearRegression()
lm.fit(X, y)

# Assign and print predictions
preds = lm.predict(X)

# Plot your fit to visualize your model
plt.scatter(X, y)
plt.plot(X, preds, color='red')
plt.show()


from sklearn.linear_model import LinearRegression 
X_train = np.array(weather['Humidity9am']).reshape(-1,1)
y_train = weather['Humidity3pm']

# Create and fit your linear regression model
lm = LinearRegression()
lm.fit(X_train, y_train)

# Assign and print predictions
preds = lm.predict(X_train)

# Assign and print coefficient 
coef = lm.coef_
print(coef)

<script.py> output:
    [62.90599123 54.20645768 43.92519074 52.6247243  51.04299093 61.32425786
     64.48772461 60.53339117 47.87952418 65.27859129 59.74252449 37.59825725
     70.8146581  74.76899154 27.31699032 38.38912393 70.8146581  48.67039087
     47.08865749 68.44205804 36.80739056 70.02379142 58.16079111 59.74252449
     62.11512455 45.50692412 28.89872369 69.23292473 59.74252449 48.67039087
     71.60552479 57.36992442 47.08865749 51.83385761 57.36992442 26.52612363
     66.86032467 32.06219044 58.16079111 47.87952418 50.25212424 57.36992442
     75.55985823 48.67039087 29.68959038 53.41559099 47.87952418 66.06945798
     51.83385761 58.16079111 43.13432406 49.46125755 76.35072491 49.46125755
     32.85305712 54.99732436 19.40832344 46.2977908  69.23292473 40.761724
     47.08865749 36.80739056 51.04299093 23.36265688 59.74252449 45.50692412
     56.57905774 51.83385761 69.23292473 66.86032467 50.25212424 36.01652387
     49.46125755 26.52612363 67.65119136 46.2977908  71.60552479 55.78819105
     56.57905774 49.46125755 50.25212424 66.06945798 68.44205804 55.78819105
     41.55259068 61.32425786 17.82659007 38.38912393 64.48772461 71.60552479
     37.59825725 64.48772461 41.55259068 62.11512455 51.83385761 71.60552479]

<script.py> output:
    [0.79086669]
    
     You can see that despite some noise in the plot, we have a decent looking fit here using Humidity9am to predict the dependent variable Humidity3pm with a linear model. Furthermore, take another look at our coefficient. This means that for every 1 unit of humidity in the morning, we can expect about 0.80 units of humidity in the afternoon. More practically, this information tells us that humidity drops about 20% from morning to afternoon!
    ______________________________________________________________________________________________________________________________
    
    Logistic regression
Let's move on to logistic regression. You'll be working with the same weather dataset again, but the goal here is to predict if it's going to rain tomorrow. We've gone ahead and created your train and test sets for you. Your dependent variables are the Humidity9am and Humidity3pm features.

It's also worth noting that the dataset has already been normalized in order to ensure that we can interpret the coefficients later on. This is always good to bring up during your interview when talking about regression for inference.

Instructions 1/3
35 XP
1
2
3
Create and fit your logistic regression model using the X_train and y_train variables.
Print the accuracy of your model using the score() function.
Let's take a look at the coefficients of our model; what does this tell you?


from sklearn.linear_model import LogisticRegression

# Create and fit your model
clf = LogisticRegression()
clf.fit(X_train, y_train.values.ravel())

from sklearn.linear_model import LogisticRegression

# Create and fit your model
clf = LogisticRegression()
clf.fit(X_train, y_train.values.ravel())

# Compute and print the accuracy
acc = clf.score(X_test, y_test)
print(acc)

from sklearn.linear_model import LogisticRegression

# Create and fit your model
clf = LogisticRegression()
clf.fit(X_train, y_train.values.ravel())

# Assign and print the coefficents
coefs = clf.coef_
print(coefs)


Since our features were normalized beforehand, we can look at the magnitude of our coefficients to tell us the importance of each independent variable. Here you can see the the second variable, Humidity3pm was much more important to our outcome than humidity from that morning. This is intuitive since we are trying to predict the rain for tomorrow!

    
    _________________________________________________________________________________________________________________________
    
 Regression evaluation
Let's revisit the linear regression model that you created with LinearRegression() and then trained with the fit() function a few exercises ago. Evaluate the performance your model, imported here as lm for you to call.

The weather data has been imported for you with the X and y variables as well, just like before. Let's get to calculating the R-squared, mean squared error, and mean absolute error values for the model.

Instructions 1/3
35 XP
1
Compute and print the R-squared score of our model using the score() function.   
  Compute and print mean squared error using the mean_squared_error() function.  
Adapt your code to compute and print the mean absolute error this time using the mean_absolute_error() function.    


# R-squared score
r2 = lm.score(X,y)
print(r2)

# Mean squared error
from sklearn.metrics import mean_squared_error
preds = lm.predict(X)
mse = mean_squared_error(y, preds)
print(mse)

# Mean absolute error
from sklearn.metrics import mean_absolute_error
preds = lm.predict(X)
mae = mean_absolute_error(y, preds)
print(mae)


<script.py> output:
    542.2291666666666

<script.py> output:
    226.12721831681654

<script.py> output:
    11.522404665934568

    Good job! Note that our R-squared value tells us the percentage of the variance of y that X is responsible for. Which error metric would you recommend for this dataset? If you remember from when you plotted your model fit, there aren't too many outliers, so mean squared error would be a good choice to go with! When interviewing, make sure you have sound reasoning to back up your choice on questions like this.
    
    
    _________________________________________________________________________________________________________________________
    
 Classification evaluation
Moving forward with evaluation metrics, this time you'll evaluate our logistic regression model from before with the goal of predicting the binary RainTomorrow feature using humidity.

We have gone ahead and imported the model as clf and the same test sets assigned to the X_test and y_test variables. Generate and analyze the confusion matrix and then compute both precision and recall before making a conclusion.

Instructions 1/3
35 XP
1
Generate and print out the confusion matrix for your model; identify the Type I and Type II errors.
Compute and print the precision of your model; can you explain why precision is helpful in this context?
Adapt your code to compute and print the recall of your model; what do you conclude?



# Generate and output the confusion matrix
from sklearn.metrics import confusion_matrix
preds = clf.predict(X_test)
matrix = confusion_matrix(y_test, preds)
print(matrix)

# Compute and print the precision
from sklearn.metrics import precision_score
preds = clf.predict(X_test)
precision = precision_score(y_test, preds)
print(precision)

# Compute and print the recall
from sklearn.metrics import recall_score
preds = clf.predict(X_test)
recall = recall_score(y_test, preds)
print(recall)


Good work! You can see here that the precision of our rain prediction model was quite high, meaning that we didn't make too many Type I errors. However, there were plenty of Type II errors shown in the bottom-left quadrant of the confusion matrix. This is indicated further by the low recall score, meaning that there were plenty of rainy days that we missed out on. Think a little about the context and what method you would choose to optimize for!


________________________________________________________________________________________________________________________________

Identifying outliers
Let's keep at it with our laptops dataset and tackle some outliers hiding away. In this exercise, we'll stick to the first technique we discussed in the slides using standard deviations to identify extreme values, since this method is more common in practice.

You'll compute the descriptive statistics and outlier boundaries, and then identify the rows with them before dropping them from the dataset. You'll be working primarily with the Price column here.


Calculate the mean and standard deviation of the Price column.
Compute and print the upper and lower boundaries for acceptable values.
Identify the rows with outliers; print outliers when you have it.

# Calculate the mean and std
mean, std = laptops['Price'].mean(), laptops['Price'].std()

# Compute and print the upper and lower threshold
cut_off = 3 * std
lower, upper = mean - cut_off, mean + cut_off
print(lower, 'to', upper)

# Identify and print rows with outliers
outliers = laptops[(laptops['Price'] > upper) | 
                   (laptops['Price'] < lower)]
print(outliers)

# Drop the rows from the dataset
laptops = laptops[(laptops['Price'] <= upper) | 
                  (laptops['Price'] >= lower)]


Good job! In this scenario, dropping the outliers was likely the right move since the values were unthinkable for laptops prices. This implies that there was some mistake in data entry or data collection. With that being said, this won't always be the best path forward. It's important to understand why you got the outliers that you did and if they provide valuable information before you throw them out.


________________________________________________________________________________________________________________________________

Visualizing the tradeoff
We know that the bias-variance tradeoff serves as the basis for dealing with issues like overfitting and underfitting in machine learning.

In this final exercise, you'll revisit our weather dataset one last time by visualizing the difference between high bias and high variance models using the already imported preds and preds2 variables.

As a reminder, we are using the Temp9am feature to predict our dependent variable, the Temp3pm feature. The usual packages have been imported.

Instructions 1/3
35 XP
1
2
3
Visualize the relationship between the imported X and y variables using the scatter() function.
Add the simple linear regression predictions saved as preds to your scatter plot; this is the model you created earlier in the chapter.



# Use X and y to create a scatterplot
plt.scatter(X, y)
plt.show()


# Use X and y to create a scatterplot
plt.scatter(X, y)

# Add your model predictions to the scatter plot 
plt.plot(np.sort(X), preds)

# Add the higher-complexity model predictions as well
plt.plot(np.sort(X), preds2)
plt.show()


Great job! This plot does an excellent job of showing both high-bias and high-variance models. Your original simple linear regression fit (the straight blue line) has higher bias, while the higher-order polynomial model is much more flexible to new observations and therefore has higher variance. In most cases, you'll find that the best result comes somewhere in between these examples - hence the tradeoff!

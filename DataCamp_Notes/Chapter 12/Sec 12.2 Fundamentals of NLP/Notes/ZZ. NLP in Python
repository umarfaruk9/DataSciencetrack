Natural Language Processing Fundamentals in Python

Course Description
In this course, you'll learn Natural Language Processing (NLP) basics, such as how to identify and separate words, how to extract topics in a text, and how to build your own fake news classifier. You'll also learn how to use basic libraries such as NLTK, alongside libraries which utilize deep learning to solve common NLP problems. This course will give you the foundation to process and parse text as you move forward in your Python learning.

===============================================================================================================================

1
Regular expressions & word tokenization
FREE
0%
This chapter will introduce some basic NLP concepts, such as word tokenization and regular expressions to help parse text. You'll also learn how to handle non-English text and more difficult tokenization you might find as you explore the wide world of NLP.

_____________________________________________________________________________________________________________________________

Introduction to regular
expressions

What is Natural Language Processing?
Field of study focused on making sense of language
Using statistics and computers
You will learn the basics of NLP
Topic identification
Text classification
NLP applications include:
Chatbots
Translation
Sentiment analysis
... and many more!



What exactly are regular expressions?
Strings with a special syntax
Allow us to match patterns in other strings
Applications of regular expressions:
Find all web links in a document
Parse email addresses, remove/replace unwanted characters

In [1]: import re
In [2]: re.match('abc'
,
'abcdef')
Out[2]: <_sre.SRE_Match object; span=(0, 3), match='abc'>
In [3]: word_regex = '\w+'
In [4]: re.match(word_regex,
'hi there!')
Out[4]: <_sre.SRE_Match object; span=(0, 2), match='hi'>


Common regex patterns (7)
pattern matches example
\w+ word 'Magic'
\d digit 9
\s space ' '
.* wildcard 'username74'
+ or * greedy match 'aaaaaa'
\S not space 'no_spaces'
[a-z] lowercase group 'abcdefg




Python's re Module
re module
split: split a string on regex
findall: find all patterns in a string
search: search for a pattern
match: match an entire string or substring based on a pattern
Pattern first, and the string second
May return an iterator, string, or match object
In [5]: re.split('\s+'
,
'Split on spaces.')
Out[5]: ['Split'
,
'on'
,
'spaces.']

________________________________________________________________________________________________________________________

Practicing regular expressions: re.split() and re.findall()
Now you'll get a chance to write some regular expressions to match digits, strings and non-alphanumeric characters. Take a look at my_string first by printing it in the IPython Shell, to determine how you might best match the different steps.

Note: It's important to prefix your regex patterns with r to ensure that your patterns are interpreted in the way you want them to. Else, you may encounter problems to do with escape sequences in strings. For example, "\n" in Python is used to indicate a new line, but if you use the r prefix, it will be interpreted as the raw string "\n" - that is, the character "\" followed by the character "n" - and not as a new line.

Instructions
100 XP
Instructions
100 XP
Import the regular expression module re.
Split my_string on each sentence ending. To do this:
Write a pattern called sentence_endings to match sentence endings (., ?, and !).
Use re.split() to split my_string on the pattern and print the result.
Find and print all capitalized words in my_string by writing a pattern called capitalized_words and using re.findall().
Remember the [a-z] pattern shown in the video to match lowercase groups? Modify that pattern appropriately in order to match uppercase groups.
Write a pattern called spaces to match one or more spaces ("\s+") and then use re.split() to split my_string on this pattern, keeping all punctuation intact. Print the result.
Find all digits in my_string by writing a pattern called digits ("\d+") and using re.findall(). Print the result.


# Import the regex module
import re

# Write a pattern to match sentence endings: sentence_endings
sentence_endings = r"[.?!]"

# Split my_string on sentence endings and print the result
print(re.split(sentence_endings, my_string))

# Find all capitalized words in my_string and print the result
capitalized_words = r"[A-Z]\w+"
print(re.findall(capitalized_words, my_string))

# Split my_string on spaces and print the result
spaces = r"\s+"
print(re.split(spaces, my_string))

# Find all digits in my_string and print the result
digits = r"\d+"
print(re.findall(digits, my_string))




________________________________________________________________________________________________________________________


Introduction to
tokenization


What is tokenization?
Turning a string or document into tokens (smaller chunks)
One step in preparing a text for NLP
Many different theories and rules
You can create your own rules using regular expressions
Some examples:
Breaking out words or sentences
Separating punctuation
Separating all hashtags in a tweet


nltk library
nltk: natural language toolkit
In [1]: from nltk.tokenize import word_tokenize
In [2]: word_tokenize("Hi there!")
Out[2]: ['Hi'
,
'there'
,
'!']


Why tokenize?
Easier to map part of speech
Matching common words
Removing unwanted tokens
"I don't like Sam's shoes."
"I"
,
"do"
,
"n't"
,
"like"
,
"Sam"
,
"'s"
,
"shoes"
,
"."


Other nltk tokenizers
sent_tokenize: tokenize a document into sentences
regexp_tokenize: tokenize a string or document based on a regular
expression pattern
TweetTokenizer: special class just for tweet tokenization, allowing you
to separate hashtags, mentions and lots of exclamation points!!!



More regex practice
Difference between re.search() and re.match()
In [1]: import re
In [2]: re.match('abc'
,
'abcde')
Out[2]: <_sre.SRE_Match object; span=(0, 3), match='abc'>
In [3]: re.search('abc'
,
'abcde')
Out[3]: <_sre.SRE_Match object; span=(0, 3), match='abc'>
In [4]: re.match('cd'
,
'abcde')
In [5]: re.search('cd'
,
'abcde')
Out[5]: <_sre.SRE_Match object; span=(2, 4), match='cd'>

_____________________________________________________________________________________________________________________


Word tokenization with NLTK
Here, you'll be using the first scene of Monty Python's Holy Grail, which has been pre-loaded as scene_one. Feel free to check it out in the IPython Shell!

Your job in this exercise is to utilize word_tokenize and sent_tokenize from nltk.tokenize to tokenize both words and sentences from Python strings - in this case, the first scene of Monty Python's Holy Grail.

Instructions
100 XP
Import the sent_tokenize and word_tokenize functions from nltk.tokenize.
Tokenize all the sentences in scene_one using the sent_tokenize() function.
Tokenize the fourth sentence in sentences, which you can access as sentences[3], using the word_tokenize() function.
Find the unique tokens in the entire scene by using word_tokenize() on scene_one and then converting it into a set using set().
Print the unique tokens found. This has been done for you, so hit 'Submit Answer' to see the results!


# Import necessary modules
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize

# Split scene_one into sentences: sentences
sentences = sent_tokenize(scene_one)

# Use word_tokenize to tokenize the fourth sentence: tokenized_sent
tokenized_sent = word_tokenize(sentences[3])

# Make a set of unique tokens in the entire scene: unique_tokens
unique_tokens = set(word_tokenize(scene_one))

# Print the unique tokens result
print(unique_tokens)


<script.py> output:
    {'master', 'seek', 'wants', 'suggesting', 'European', 'climes', 'an', '[', 'Not', 'Please', 'ounce', 'all', 'Court', 'anyway', "'ve", 'matter', 'feathers', 'may', 'here', 'castle', 'weight', 'Mercea', 'forty-three', 'they', ',', 'son', 'using', 'martin', 'times', 'point', 'your', 'its', 'minute', 'two', 'by', 'simple', 'lord', 'ratios', 'It', 'does', 'question', 'through', 'non-migratory', 'but', 'defeator', 'warmer', 'goes', 'Whoa', 'guiding', 'beat', 'you', '...', 'Oh', 'We', 'So', '--', 'not', 'needs', 'must', '?', 'a', 'They', 'why', 'use', 'halves', 'Listen', 'will', 'maintain', 'migrate', 'then', ']', 'temperate', 'line', 'and', 'That', 'swallow', 'breadth', 'one', 'Well', 'agree', 'under', 'pound', 'snows', 'them', "'s", 'do', 'England', 'go', 'wings', 'second', 'our', 'got', 'Yes', 'since', 'African', 'five', 'my', 'grip', 'course', 'yeah', 'Saxons', 'me', 'get', 'strand', 'at', ':', 'air-speed', 'bangin', 'mean', 'he', 'of', 'order', 'KING', 'zone', 'Pull', 'with', 'Supposing', 'creeper', "n't", 'A', 'Will', 'if', 'ask', 'search', 'am', '!', 'other', 'carried', 'coconuts', '#', 'Uther', 'strangers', 'be', 'coconut', 'I', 'dorsal', 'just', 'back', 'held', 'empty', 'SOLDIER', 'bird', 'plover', 'who', 'tell', 'Arthur', 'court', 'Camelot', 'right', 'sovereign', 'wind', 'Who', 'yet', 'south', '2', 'together', 'could', 'is', 'Patsy', 'there', 'or', 'bring', 'Wait', 'No', 'are', '.', 'The', 'it', 'length', 'the', 'Are', 'kingdom', 'SCENE', 'carrying', 'land', 'velocity', 'But', 'You', 'this', 'winter', 'from', "'em", 'to', 'Am', '1', "'m", 'house', 'swallows', 'grips', 'husk', 'servant', "'d", 'sun', 'covered', 'Halt', 'join', 'Pendragon', 'Britons', 'tropical', 'fly', 'King', 'have', 'Where', 'maybe', "'", 'ARTHUR', 'Found', 'every', 'found', 'in', 'where', 'knights', 'ridden', 'interested', 'Ridden', 'carry', 'In', 'these', 'on', 'horse', 'trusty', 'that', 'What', 'clop', 'speak', "'re"}

________________________________________________________________________________________________________________________

More regex with re.search()
In this exercise, you'll utilize re.search() and re.match() to find specific tokens. Both search and match expect regex patterns, similar to those you defined in an earlier exercise. You'll apply these regex library methods to the same Monty Python text from the nltk corpora.

You have both scene_one and sentences available from the last exercise; now you can use them with re.search() and re.match() to extract and match more text.

Instructions
100 XP
Use re.search() to search for the first occurance of the word "coconuts" in scene_one. Store the result in match.
Print the start and end indexes of match using its .start() and .end() methods, respectively.
Write a regular expression called pattern1 to find anything in square brackets.
Use re.search() with the previous pattern to find the first text in square brackets in the scene. Print the result.
Use re.match() to match the script notation in the fourth line (ARTHUR:) and print the result. The tokenized sentences of scene_one are available in your namespace as sentences.


# Search for the first occurrence of "coconuts" in scene_one: match
match = re.search("coconuts", scene_one)

# Print the start and end indexes of match
print(match.start(), match.end())

# Write a regular expression to search for anything in square brackets: pattern1
pattern1 = r"\[.*\]"

# Use re.search to find the first text in square brackets
print(re.search(pattern1, scene_one))

# Find the script notation at the beginning of the fourth sentence and print it
pattern2 = r"[\w\s]+:"
print(re.match(pattern2, sentences[3]))



________________________________________________________________________________________________________________________

Advanced tokenization
with regex

Regex groups using or "|"
OR is represented using |
You can define a group using ()
You can define explicit character ranges using []

In [1]: import re
In [2]: match_digits_and_words = ('(\d+|\w+)')
In [3]: re.findall(match_digits_and_words,
'He has 11 cats.')
Out[3]: ['He'
,
'has'
,
'11'
,
'cats']



Regex ranges and groups
pattern matches example
[A-Za-z]+ upper and lowercase English alphabet 'ABCDEFghijk'
[0-9] numbers from 0 to 9 9
[A-Za-z\-
\.]+
upper and lowercase English alphabet, -
and .
'MyWebsite.com'
(a-z) a, - and z 'a-z'
(\s+l,) spaces or a comma '
,
'



Character range with re.match()
In [1]: import re
In [2]: my_str = 'match lowercase spaces nums like 12, but no commas'
In [3]: re.match('[a-z0-9 ]+'
, my_str)
Out[3]: <_sre.SRE_Match object;
span=(0, 42), match='match lowercase spaces nums like 12'>

________________________________________________________________________________________________________________________

Regex with NLTK tokenization
Twitter is a frequently used source for NLP text and tasks. In this exercise, you'll build a more complex tokenizer for tweets with hashtags and mentions using nltk and regex. The nltk.tokenize.TweetTokenizer class gives you some extra methods and attributes for parsing tweets.

Here, you're given some example tweets to parse using both TweetTokenizer and regexp_tokenize from the nltk.tokenize module. These example tweets have been pre-loaded into the variable tweets. Feel free to explore it in the IPython Shell!

Instructions
100 XP
Instructions
100 XP
From nltk.tokenize, import regexp_tokenize and TweetTokenizer.
A regex pattern to define hashtags called pattern1 has been defined for you. Call regexp_tokenize() with this hashtag pattern on the first tweet in tweets.
Write a new pattern called pattern2 to match mentions and hashtags. A mention is something like @DataCamp. Then, call regexp_tokenize() with your new hashtag pattern on the last tweet in tweets. You can access the last element of a list using -1 as the index, for example, tweets[-1].
Create an instance of TweetTokenizer called tknzr and use it inside a list comprehension to tokenize each tweet into a new list called all_tokens. To do this, use the .tokenize() method of tknzr, with t as your iterator variable.


# Import the necessary modules
from nltk.tokenize import regexp_tokenize
from nltk.tokenize import TweetTokenizer

# Define a regex pattern to find hashtags: pattern1
pattern1 = r"#\w+"

# Use the pattern on the first tweet in the tweets list
regexp_tokenize(tweets[0], pattern1)

# Write a pattern that matches both mentions and hashtags
pattern2 = r"([@]\w+)"

# Use the pattern on the last tweet in the tweets list
regexp_tokenize(tweets[-1], pattern2)

# Use the TweetTokenizer to tokenize all tweets into one list
tknzr = TweetTokenizer()
all_tokens = [tknzr.tokenize(t) for t in tweets]
print(all_tokens)


<script.py> output:
    [['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]
    
________________________________________________________________________________________________________________________


Non-ascii tokenization
In this exercise, you'll practice advanced tokenization by tokenizing some non-ascii based text. You'll be using German with emoji!

Here, you have access to a string called german_text, which has been printed for you in the Shell. Notice the emoji and the German characters!

The following modules have been pre-imported from nltk.tokenize: regexp_tokenize and word_tokenize.

Unicode ranges for emoji are:

('\U0001F300'-'\U0001F5FF'), ('\U0001F600-\U0001F64F'), ('\U0001F680-\U0001F6FF'), and ('\u2600'-\u26FF-\u2700-\u27BF').

Instructions
100 XP
Tokenize all the words in german_text using word_tokenize(), and print the result.
Tokenize only the capital words in german_text.
First, write a pattern called capital_words to match only capital words. Make sure to check for the German Ü!
Then, tokenize it using regexp_tokenize().
Tokenize only the emoji in german_text. The pattern using the unicode ranges for emoji given in the assignment text has been written for you. Your job is to use regexp_tokenize() to tokenize the emoji.

# Tokenize and print all words in german_text
all_words = word_tokenize(german_text)
print(all_words)

# Tokenize and print only capital words
capital_words = r"[A-ZÜ]\w+"
print(regexp_tokenize(german_text, capital_words))

# Tokenize and print only emoji
emoji = "['\U0001F300-\U0001F5FF'|'\U0001F600-\U0001F64F'|'\U0001F680-\U0001F6FF'|'\u2600-\u26FF\u2700-\u27BF']"
print(regexp_tokenize(german_text, emoji))

<script.py> output:
    ['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', '🍕', 'Und', 'fährst', 'du', 'mit', 'Über', '?', '🚕']
    ['Wann', 'Pizza', 'Und', 'Über']
    ['🍕', '🚕']




________________________________________________________________________________________________________________________

Charting word length
with nltk


Getting started with matplotlib
Charting library used by many open source Python projects
Straightforward functionality with lots of options
Histograms
Bar charts
Line charts
Scatter plots
... and also advanced functionality like 3D graphs and animations!


Plotting a histogram with matplotlib
In [1]: from matplotlib import pyplot as plt
In [2]: plt.hist([1, 5, 5, 7, 7, 7, 9])
Out[2]: (array([ 1., 0., 0., 0., 0., 2., 0., 3., 0., 1.]),
array([ 1. , 1.8, 2.6, 3.4, 4.2, 5. , 5.8, 6.6,
7.4, 8.2, 9. ]),
<a list of 10 Patch objects>)
In [3]: plt.show()


Generated Histogram


Combining NLP data extraction with plotting
In [1]: from matplotlib import pyplot as plt
In [2]: from nltk.tokenize import word_tokenize
In [3]: words = word_tokenize("This is a pretty cool tool!")
In [4]: word_lengths = [len(w) for w in words]
In [5]: plt.hist(word_lengths)
Out[5]: (array([ 2., 0., 1., 0., 0., 0., 3., 0., 0., 1.]),
array([ 1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5, 5. , 5.5,
6. ]),
<a list of 10 Patch objects>)
In [6]: plt.show()









________________________________________________________________________________________________________________________

# Split the script into lines: lines
lines = holy_grail.split('\n')

# Replace all script lines for speaker
pattern = "[A-Z]{2,}(\s)?(#\d)?([A-Z]{2,})?:"
lines = [re.sub(pattern, '', l) for l in lines]

# Tokenize each line: tokenized_lines
tokenized_lines = [regexp_tokenize(s,"\w+") for s in lines]

# Make a frequency list of lengths: line_num_words
line_num_words = [len(t_line) for t_line in tokenized_lines]

# Plot a histogram of the line lengths
plt.hist(line_num_words)

# Show the plot
plt.show()


Charting practice
Try using your new skills to find and chart the number of words per line in the script using matplotlib. The Holy Grail script is loaded for you, and you need to use regex to find the words per line.

Using list comprehensions here will speed up your computations. For example: my_lines = [tokenize(l) for l in lines] will call a function tokenize on each line in the list lines. The new transformed list will be saved in the my_lines variable.

You have access to the entire script in the variable holy_grail. Go for it!

Instructions
70 XP
Split the script into lines using the newline ('\n') character.
Use re.sub() inside a list comprehension to replace the prompts such as ARTHUR: and SOLDIER #1. The pattern has been written for you.
Use a list comprehension to tokenize lines with regexp_tokenize(), keeping only words. Recall that the pattern for words is "\w+".
Use a list comprehension to create a list of line lengths called line_num_words.
Use t_line as your iterator variable to iterate over tokenized_lines, and then len() function to compute line lengths.
Plot a histogram of line_num_words using plt.hist(). Don't forgot to use plt.show() as well to display the plot.


Hint
Use the .split() method on holy_grail with the newline character ('\n') as the argument.
Recall that re.sub() requires 3 arguments: The pattern, the replacement, and the string. The pattern is given for you; the replacement is '' and the string is l.
Use regexp_tokenize() as the output expression of your list comprehension, with s and "\w+" as the arguments.
To create line_num_words, use len(t_line) as the output expression of the list comprehension.
Use plt.hist() with line_num_words as the argument to create the histogram, and then plt.show() to display it.


===============================================================================================================================

2
Simple topic identification
0%
This chapter will introduce you to topic identification, which you can apply to any text you encounter in the wild. Using basic NLP models, you will identify topics from texts based on term frequencies. You'll experiment and compare two simple methods - bag-of-words and Tf-idf using NLTK and a new library - Gensim.

Word counts with bagof-words
Bag-of-words
Basic method for finding topics in a text
Need to first create tokens using tokenization
... and then count up all the tokens
The more frequent a word, the more important it might be
Can be a great way to determine the significant words in a text


Bag-of-words example
Text: "The cat is in the box. The cat likes the box. The box is over the
cat."
Bag of words (stripped punctuation):
"The": 3,
"box": 3
"cat": 3,
"the": 3
"is": 2
"in": 1,
"likes": 1,
"over": 1


Bag-of-words in Python
In [1]: from nltk.tokenize import word_tokenize
In [2]: from collections import Counter
In [3]: Counter(word_tokenize(
"""The cat is in the box. The cat likes the box.
The box is over the cat."""))
Out[3]:
Counter({'.': 3,
'The': 3,
'box': 3,
'cat': 3,
'in': 1,
...
'the': 3})
In [4]: counter.most_common(2)
Out[4]: [('The'
, 3), ('box'
, 3)]


_______________________________________________________________________________________________________________

Building a Counter with bag-of-words
In this exercise, you'll build your first (in this course) bag-of-words counter using a Wikipedia article, which has been pre-loaded as article. Try doing the bag-of-words without looking at the full article text, and guessing what the topic is! If you'd like to peek at the title at the end, we've included it as article_title. Note that this article text has had very little preprocessing from the raw Wikipedia database entry.

word_tokenize has been imported for you.

Instructions
100 XP
Import Counter from collections.
Use word_tokenize() to split the article into tokens.
Use a list comprehension with t as the iterator variable to convert all the tokens into lowercase. The .lower() method converts text into lowercase.
Create a bag-of-words counter called bow_simple by using Counter() with lower_tokens as the argument.
Use the .most_common() method of bow_simple to print the 10 most common tokens.


# Import Counter
from collections import Counter

# Tokenize the article: tokens
tokens = word_tokenize(article)

# Convert the tokens into lowercase: lower_tokens
lower_tokens = [t.lower() for t in tokens]

# Create a Counter with the lowercase tokens: bow_simple
bow_simple = Counter(lower_tokens)

# Print the 10 most common tokens
print(bow_simple.most_common(10))


<script.py> output:
    [(',', 151), ('the', 150), ('.', 89), ('of', 81), ("''", 68), ('to', 63), ('a', 60), ('in', 44), ('and', 41), ('(', 40)]
    
     
___________________________________________________________________________________________________________________________

Simple text
preprocessing


Why preprocess?
Helps make for better input data
When performing machine learning or other statistical methods
Examples:
Tokenization to create a bag of words
Lowercasing words
Lemmatization/Stemming
Shorten words to their root stems
Removing stop words, punctuation, or unwanted tokens
Good to experiment with different approaches


Preprocessing example
Input text: Cats, dogs and birds are common pets. So are fish.
Output tokens: cat, dog, bird, common, pet, fish



Text preprocessing with Python
In [1]: from ntlk.corpus import stopwords
In [2]: text = """The cat is in the box. The cat likes the box.
The box is over the cat."""
In [3]: tokens = [w for w in word_tokenize(text.lower())
if w.isalpha()]
In [4]: no_stops = [t for t in tokens
if t not in stopwords.words('english')]
In [5]: Counter(no_stops).most_common(2)
Out[5]: [('cat'
, 3), ('box'
, 3)]

_________________________________________________________________________________________________________________________


Text preprocessing practice
Now, it's your turn to apply the techniques you've learned to help clean up text for better NLP results. You'll need to remove stop words and non-alphabetic characters, lemmatize, and perform a new bag-of-words on your cleaned text.

You start with the same tokens you created in the last exercise: lower_tokens. You also have the Counter class imported.

Instructions
100 XP
Import the WordNetLemmatizer class from nltk.stem.
Create a list called alpha_only that iterates through lower_tokens and retains only alphabetical characters. You can use the .isalpha() method to check for this.
Create another list called no_stops in which you remove all stop words, which are held in a list called english_stops.
Initialize a WordNetLemmatizer object called wordnet_lemmatizer and use its .lemmatize() method on the tokens in no_stops to create a new list called lemmatized.
Finally, create a new Counter called bow with the lemmatized words and show the 10 most common tokens.


# Import WordNetLemmatizer
from nltk.stem import WordNetLemmatizer

# Retain alphabetic words: alpha_only
alpha_only = [t for t in lower_tokens if t.isalpha()]

# Remove all stop words: no_stops
no_stops = [t for t in alpha_only if t not in english_stops]

# Instantiate the WordNetLemmatizer
wordnet_lemmatizer = WordNetLemmatizer()

# Lemmatize all tokens into a new list: lemmatized
lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]

# Create the bag-of-words: bow
bow = Counter(lemmatized)

# Print the 10 most common tokens
print(bow.most_common(10))

<script.py> output:
    [('debugging', 40), ('system', 25), ('software', 16), ('bug', 16), ('problem', 15), ('tool', 15), ('computer', 14), ('process', 13), ('term', 13), ('used', 12)]

__________________________________________________________________________________________________________

Introduction to gensim
What is gensim?
Popular open-source NLP library
Uses top academic models to perform complex tasks
Building document or word vectors
Performing topic identification and document comparison


What is a word vector?



Creating a gensim dictionary
In [1]: from gensim.corpora.dictionary import Dictionary
In [2]: from nltk.tokenize import word_tokenize
In [3]: my_documents = ['The movie was about a spaceship and aliens.'
,
...: 'I really liked the movie!'
,
...: 'Awesome action scenes, but boring characters.'
,
...: 'The movie was awful! I hate alien films.'
,
...: 'Space is cool! I liked the movie.'
,
...: 'More space films, please!'
,]
In [4]: tokenized_docs = [word_tokenize(doc.lower())
...: for doc in my_documents]
In [5]: dictionary = Dictionary(tokenized_docs)
In [6]: dictionary.token2id
Out[6]:
{'!': 11,
'
,
': 17,
'.': 7,
'a': 2,
'about': 4,
...
}



Creating a gensim corpus
In [7]: corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]
In [8]: corpus
Out[8]:
[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)],
[(0, 1), (1, 1), (9, 1), (10, 1), (11, 1), (12, 1)],
...
]

gensim models can be easily saved, updated, and reused
Our dictionary can also be updated
This more advanced and feature rich bag-of-words can be used in
future exercises

______________________________________________________________________________________________________________________________

Creating and querying a corpus with gensim
It's time to apply the methods you learned in the previous video to create your first gensim dictionary and corpus!

You'll use these data structures to investigate word trends and potential interesting topics in your document set. To get started, we have imported a few additional messy articles from Wikipedia, which were preprocessed by lowercasing all words, tokenizing them, and removing stop words and punctuation. These were then stored in a list of document tokens called articles. You'll need to do some light preprocessing and then generate the gensim dictionary and corpus.

Instructions
100 XP
Instructions
100 XP
Import Dictionary from gensim.corpora.dictionary.
Initialize a gensim Dictionary with the tokens in articles.
Obtain the id for "computer" from dictionary. To do this, use its .token2id method which returns ids from text, and then chain .get() which returns tokens from ids. Pass in "computer" as an argument to .get().
Use a list comprehension in which you iterate over articles to create a gensim MmCorpus from dictionary.
In the output expression, use the .doc2bow() method on dictionary with article as the argument.
Print the first 10 word ids with their frequency counts from the fifth document. This has been done for you, so hit 'Submit Answer' to see the results!

# Import Dictionary
from gensim.corpora.dictionary import Dictionary

# Create a Dictionary from the articles: dictionary
dictionary = Dictionary(articles)

# Select the id for "computer": computer_id
computer_id = dictionary.token2id.get("computer")

# Use computer_id with the dictionary to print the word
print(dictionary.get(computer_id))

# Create a MmCorpus: corpus
corpus = [dictionary.doc2bow(article) for article in articles]

# Print the first 10 word ids with their frequency counts from the fifth document
print(corpus[4][:10])

<script.py> output:
    computer
    [(0, 88), (23, 11), (24, 2), (39, 1), (41, 2), (55, 22), (56, 1), (57, 1), (58, 1), (59, 3)]
________________________________________________________________________________________________
Gensim bag-of-words
Now, you'll use your new gensim corpus and dictionary to see the most common terms per document and across all documents. You can use your dictionary to look up the terms. Take a guess at what the topics are and feel free to explore more documents in the IPython Shell!

You have access to the dictionary and corpus objects you created in the previous exercise, as well as the Python defaultdict and itertools to help with the creation of intermediate data structures for analysis.

The fifth document from corpus is stored in the variable doc, which has been sorted in descending order.

Instructions
100 XP
Instructions
100 XP
Print the top five words of bow_doc using each word_id with the dictionary alongside word_count. The word_id can be accessed using the .get() method of dictionary.
Create a defaultdict called total_word_count in which the keys are all the token ids (word_id) and the values are the sum of their occurrence across all documents (word_count). Remember to specify int when creating the defaultdict, and inside the for loop, increment each word_id of total_word_count by word_count.
Create a sorted list from the defaultdict, using words across the entire corpus. To achieve this, use the .items() method on total_word_count inside sorted().
Similar to how you printed the top five words of bow_doc earlier, print the top five words of sorted_word_count as well as the number of occurrences of each word across all the documents.

# Save the fifth document: doc
doc = corpus[4]

# Sort the doc for frequency: bow_doc
bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)

# Print the top 5 words of the document alongside the count
for word_id, word_count in bow_doc[:5]:
    print(dictionary.get(word_id), word_count)
    
# Create the defaultdict: total_word_count
total_word_count = defaultdict(int)
for word_id, word_count in itertools.chain.from_iterable(corpus):
    total_word_count[word_id] += word_count

# Create a sorted list from the defaultdict: sorted_word_count 
sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) 

# Print the top 5 words across all documents alongside the count
for word_id, word_count in sorted_word_count[:5]:
    print(dictionary.get(word_id), word_count)



________________________________________________________________________________________________

Tf-idf with gensim
What is tf-idf?
Term frequency - inverse document frequency
Allows you to determine the most important words in each document
Each corpus may have shared words beyond just stopwords
These words should be down-weighted in importance
Example from astronomy: "Sky"
Ensures most common words don't show up as key words
Keeps document specific frequent words weighted high





Tf-idf formula
w = tf ∗ log( )
w = tf-idf weight for token i in document j
tf = number of occurences of token i in document j
df = number of documents that contain token i
N = total number of documents



Tf-idf with gensim
In [10]: from gensim.models.tfidfmodel import TfidfModel
In [11]: tfidf = TfidfModel(corpus)
In [12]: tfidf[corpus[1]]
Out[12]:
[(0, 0.1746298276735174),
(1, 0.1746298276735174),
(9, 0.29853166221463673),
(10, 0.7716931521027908),
...
]

________________________________________________________________________________________________

Tf-idf with Wikipedia
Now it's your turn to determine new significant terms for your corpus by applying gensim's tf-idf. You will again have access to the same corpus and dictionary objects you created in the previous exercises - dictionary, corpus, and doc. Will tf-idf make for more interesting results on the document level?

Instructions
0 XP
Import TfidfModel from gensim.models.tfidfmodel.
Initialize a new TfidfModel called tfidf using corpus.
Use doc to calculate the weights. You can do this by passing [doc] to tfidf.
Print the first five term ids with weights.
Sort the term ids and weights in a new list from highest to lowest weight. This has been done for you.
Print the top five weighted words (term_id) from sorted_tfidf_weights along with their weighted score (weight).


# Import TfidfModel
from gensim.models.tfidfmodel import TfidfModel

# Create a new TfidfModel using the corpus: tfidf
tfidf = TfidfModel(corpus)

# Calculate the tfidf weights of doc: tfidf_weights
tfidf_weights = tfidf[doc]

# Print the first five weights
print(tfidf_weights[:5])

# Sort the weights from highest to lowest: sorted_tfidf_weights
sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)

# Print the top 5 weighted words
for term_id, weight in sorted_tfidf_weights[:5]:
    print(dictionary.get(term_id), weight)

===============================================================================================================================
3
Named-entity recognition
0%
This chapter will introduce a slightly more advanced topic - Named-entity recognition. You'll learn how to identify the who, what and where of your texts using pre-trained models on English and non-English text. You'll also learn how to use some new libraries - polyglot and spaCy - to add to your NLP toolbox.


What is Named Entity Recognition?
NLP task to identify important named entities in the text
People, places, organizations
Dates, states, works of art
... and other categories!
Can be used alongside topic identification
... or on its own!
Who? What? When? Where?



nltk and the Stanford CoreNLP Library
The Stanford CoreNLP library:
Integrated into Python via nltk
Java based
Support for NER as well as coreference and dependency trees

Using nltk for Named Entity Recognition
In [1]: import nltk
In [2]: sentence = '''In New York, I like to ride the Metro to visit MOMA
and some restaurants rated well by Ruth Reichl.'''
In [3]: tokenized_sent = nltk.word_tokenize(sentence)
In [4]: tagged_sent = nltk.pos_tag(tokenized_sent)
In [5]: tagged_sent[:3]
Out[5]: [('In'
,
'IN'), ('New'
,
'NNP'), ('York'
,
'NNP')



nlt
k
&
#
3
9
;
s
n
e
_
c
h
u
n
k
(
)
I
n
[
6
]
:
p
r
i
n
t
(
n
l
t
k
.
n
e
_
c
h
u
n
k
(
t
a
g
g
e
d
_
s
e
n
t
)
)
(
S
I
n
/
I
N
(
G
P
E
N
e
w
/
N
N
P
Y
o
r
k
/
N
N
P
)
,
/
,
I
/
P
R
P
l
i
k
e
/
V
B
P
t
o
/
T
O
r
i
d
e
/
V
B
t
h
e
/
D
T
(
O
R
G
A
N
I
Z
A
T
I
O
N
M
e
t
r
o
/
N
N
P
)
t
o
/
T
O
v
i
s
i
t
/
V
B
(
O
R
G
A
N
I
Z
A
T
I
O
N
M
O
M
A
/
N
N
P
)
a
n
d
/
C
C
s
o
m
e
/
D
T
r
e
s
t
a
u
r
a
n
t
s
/
N
N
S
r
a
t
e
d
/
V
B
N
w
e
l
l
/
R
B
b
y
/
I
N
(
P
E
R
S
O
N
R
u
t
h
/
N
N
P
R
e
i
c
h
l
/
N
N
P
)
.
/
.
)
________________________________________________________________________________________________
NER with NLTK
You're now going to have some fun with named-entity recognition! A scraped news article has been pre-loaded into your workspace. Your task is to use nltk to find the named entities in this article.

What might the article be about, given the names you found?

Along with nltk, sent_tokenize and word_tokenize from nltk.tokenize have been pre-imported.

Instructions
100 XP
Instructions
100 XP
Tokenize article into sentences.
Tokenize each sentence in sentences into words using a list comprehension.
Inside a list comprehension, tag each tokenized sentence into parts of speech using nltk.pos_tag().
Chunk each tagged sentence into named-entity chunks using nltk.ne_chunk_sents(). Along with pos_sentences, specify the additional keyword argument binary=True.
Loop over each sentence and each chunk, and test whether it is a named-entity chunk by testing if it has the attribute label, and if the chunk.label() is equal to "NE". If so, print that chunk.


# Tokenize the article into sentences: sentences
sentences = nltk.sent_tokenize(article)

# Tokenize each sentence into words: token_sentences
token_sentences = [nltk.word_tokenize(sent) for sent in sentences]

# Tag each tokenized sentence into parts of speech: pos_sentences
pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] 

# Create the named entity chunks: chunked_sentences
chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)

# Test for stems of the tree with 'NE' tags
for sent in chunked_sentences:
    for chunk in sent:
        if hasattr(chunk, "label") and chunk.label() == "NE":
            print(chunk)


<script.py> output:
    (NE Uber/NNP)
    (NE Beyond/NN)
    (NE Apple/NNP)
    (NE Uber/NNP)
    (NE Uber/NNP)
    (NE Travis/NNP Kalanick/NNP)
    (NE Tim/NNP Cook/NNP)
    (NE Apple/NNP)
    (NE Silicon/NNP Valley/NNP)
    (NE CEO/NNP)
    (NE Yahoo/NNP)
    (NE Marissa/NNP Mayer/NNP)

________________________________________________________________________________________________

Charting practice
In this exercise, you'll use some extracted named entities and their groupings from a series of newspaper articles to chart the diversity of named entity types in the articles.

You'll use a defaultdict called ner_categories, with keys representing every named entity group type, and values to count the number of each different named entity type. You have a chunked sentence list called chunked_sentences similar to the last exercise, but this time with non-binary category names.

You can use hasattr() to determine if each chunk has a 'label' and then simply use the chunk's .label() method as the dictionary key.

Instructions
100 XP
Instructions
100 XP
Create a defaultdict called ner_categories, with the default type set to int.
Fill up the dictionary with values for each of the keys. Remember, the keys will represent the label().
In the outer for loop, iterate over chunked_sentences, using sent as your iterator variable.
In the inner for loop, iterate over sent. If the condition is true, increment the value of each key by 1.
For the pie chart labels, create a list called labels from the keys of ner_categories, which can be accessed using .keys().
Use a list comprehension to create a list called values, using the .get() method on ner_categories to compute the values of each label l.
Use plt.pie() to create a pie chart for each of the NER categories. Along with values and labels=labels, pass the extra keyword arguments autopct='%1.1f%%' and startangle=140 to add percentages to the chart and rotate the initial start angle.
Display your pie chart. Was the distribution what you expected?


# Create the defaultdict: ner_categories
ner_categories = defaultdict(int)

# Create the nested for loop
for sent in chunked_sentences:
    for chunk in sent:
        if hasattr(chunk, 'label'):
            ner_categories[chunk.label()] += 1
            
# Create a list from the dictionary keys for the chart labels: labels
labels = list(ner_categories.keys())

# Create a list of the values: values
values = [ner_categories.get(l) for l in labels]

# Create the pie chart
plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)

# Display the chart
plt.show()



Hint
Use defaultdict(int) to create ner_categories.
If the condition inside the nested for loop is true, increment ner_categories[chunk.label()].
Use the .keys() method on ner_categories to access its keys.
Use the .get() method on ner_categories with l as the argument to compute the values.
Use plt.pie() with the arguments specified in the instructions to create the pie chart. Then, use plt.show() to display it.
________________________________________________________________________________________________
Introduction to SpaCy


What is SpaCy?
NLP library similar to gensim, with different implementations
Focus on creating NLP pipelines to generate models and corpora
Open-source, with extra libraries and tools
Displacy


Displacy entity recognition visualizer
(source: https://demos.explosion.ai/displacy-e


SpaCy NER
In [1]: import spacy
In [2]: nlp = spacy.load('en')
In [3]: nlp.entity
Out[3]: <spacy.pipeline.EntityRecognizer at 0x7f76b75e68b8>
In [4]: doc = nlp("""Berlin is the capital of Germany;
and the residence of Chancellor Angela Merkel.""")
In [5]: doc.ents
Out[5]: (Berlin, Germany, Angela Merkel)
In [6]: print(doc.ents[0], doc.ents[0].label_)
Berlin GPE


Why use SpaCy for NER?
Easy pipeline creation
Different entity types compared to nltk
Informal language corpora
Easily find entities in Tweets and chat messages
Quickly growing!

________________________________________________________________________________________________

Comparing NLTK with spaCy NER
Using the same text you used in the first exercise of this chapter, you'll now see the results using spaCy's NER annotator. How will they compare?

The article has been pre-loaded as article. To minimize execution times, you'll be asked to specify the keyword arguments tagger=False, parser=False, matcher=False when loading the spaCy model, because you only care about the entity in this exercise.

Instructions
100 XP
Import spacy.
Load the 'en' model using spacy.load(). Specify the additional keyword arguments tagger=False, parser=False, matcher=False.
Create a spacy document object by passing article into nlp().
Using ent as your iterator variable, iterate over the entities of doc and print out the labels (ent.label_) and text (ent.text).


# Import spacy
import spacy

# Instantiate the English model: nlp
nlp = spacy.load('en',tagger=False, parser=False, matcher=False)

# Create a new document: doc
doc = nlp(article)

# Print all of the found entities and their labels
for ent in doc.ents:
    print(ent.label_, ent.text)


<script.py> output:
    ORG Uber
    ORG Uber
    ORG Apple
    ORG Uber
    ORG Uber
    PERSON Travis Kalanick
    ORG Uber
    PERSON Tim Cook
    ORG Apple
    CARDINAL Millions
    ORG Uber
    GPE drivers’
    LOC Silicon Valley’s
    ORG Yahoo
    PERSON Marissa Mayer
    MONEY $186m
	
________________________________________________________________________________________________



Multilingual NER with
polyglot

What is polyglot?
NLP library which uses word
vectors
Why polyglot?
Vectors for many different
languages
More than 130!

Spanish NER with polyglot

In [1]: from polyglot.text import Text
In [2]: ẗext = """El presidente de la Generalitat de Cataluña,
Carles Puigdemont, ha afirmado hoy a la alcaldesa
de Madrid, Manuela Carmena, que en su etapa de
alcalde de Girona (de julio de 2011 a enero de 2016)
hizo una gran promoción de Madrid."""
In [3]: ptext = Text(text)
In [4]: ptext.entities
Out[4]:
[I-ORG(['Generalitat'
,
'de']),
I-LOC(['Generalitat'
,
'de'
,
'Cataluña']),
I-PER(['Carles'
,
'Puigdemont']),
I-LOC(['Madrid']),
I-PER(['Manuela'
,
'Carmena']),
I-LOC(['Girona']),
I-LOC(['Madrid'])]

________________________________________________________________________________________________

French NER with polyglot I
In this exercise and the next, you'll use the polyglot library to identify French entities. The library functions slightly differently than spacy, so you'll use a few of the new things you learned in the last video to display the named entity text and category.

You have access to the full article string in article. Additionally, the Text class of polyglot has been imported from polyglot.text.

Instructions
100 XP
Create a new Text object called txt.
Iterate over txt.entities and print each entity, ent.
Print the type() of ent.


# Create a new text object using Polyglot's Text class: txt
txt = Text(article)

# Print each of the entities found
for ent in txt.entities:
    print(ent)
    
# Print the type of ent
print(type(ent))


<script.py> output:
    ['Charles', 'Cuvelliez']
    ['Charles', 'Cuvelliez']
    ['Bruxelles']
    ['l’IA']
    ['Julien', 'Maldonato']
    ['Deloitte']
    ['Ethiquement']
    ['l’IA']
    ['.']
    <class 'polyglot.text.Chunk'>

________________________________________________________________________________________________

French NER with polyglot II
Here, you'll complete the work you began in the previous exercise.

Your task is to use a list comprehension to create a list of tuples, in which the first element is the entity tag, and the second element is the full string of the entity text.

Instructions
100 XP
Use a list comprehension to create a list of tuples called entities.
The output expression of your list comprehension should be a tuple. Remember to use () to create the tuple.
The first element of each tuple is the entity tag, which you can access using its .tag attribute.
The second element is the full string of the entity text, which you can access using ' '.join(ent).
Your iterator variable should be ent, and you should iterate over all of the entities of the polyglot Text object, txt.
Print entities to see what you've created.


# Create the list of tuples: entities
entities = [(ent.tag,' '.join(ent)) for ent in txt.entities]

# Print entities
print(entities)


<script.py> output:
    [('I-PER', 'Charles Cuvelliez'), ('I-PER', 'Charles Cuvelliez'), ('I-ORG', 'Bruxelles'), ('I-PER', 'l’IA'), ('I-PER', 'Julien Maldonato'), ('I-ORG', 'Deloitte'), ('I-PER', 'Ethiquement'), ('I-LOC', 'l’IA'), ('I-PER', '.')]

________________________________________________________________________________________________

Spanish NER with polyglot
You'll continue your exploration of polyglot now with some Spanish annotation. This article is not written by a newspaper, so it is your first example of a more blog-like text. How do you think that might compare when finding entities?

The Text object has been created as txt, and each entity has been printed, as you can see in the IPython Shell.

Your specific task is to determine how many of the entities contain the words "Márquez" or "Gabo" - these refer to the same person in different ways!

Instructions
100 XP
Iterate over all of the entities of txt, using ent as your iterator variable.
Check whether the entity contains "Márquez" or "Gabo". If it does, increment count.
Hit 'Submit Answer' to see what percentage of entities refer to Gabriel García Márquez (aka Gabo).

# Initialize the count variable: count
count = 0

# Iterate over all the entities
for ent in txt.entities:
    # Check whether the entity contains 'Márquez' or 'Gabo'
    if "Márquez" in ent or "Gabo" in ent:
        # Increment count
        count += 1

# Print count
print(count)

# Calculate the percentage of entities that refer to "Gabo": percentage
percentage = count / len(txt.entities)
print(percentage)


<script.py> output:
    29
    0.29591836734693877



===============================================================================================================================


4
Building a "fake news" classifier
0%
Here, you'll apply the basics of what you've learned along with some supervised machine learning to build a "fake news" detector. You'll begin by learning the basics of supervised machine learning, and then move forward by choosing a few important features and testing ideas to identify and classify "fake news" articles.


________________________________________________________________________________________________

What is supervised learning?
Form of machine learning
Problem has predefined training data
This data has a label (or outcome) you want the model to learn
Classification problem
Goal: Make good hypotheses about the species based on



Supervised learning with NLP
Need to use language instead of geometric features
scikit-learn: Powerful open-source library
How to create supervised learning data from text?
Use bag-of-words models or tf-idf as features


IMDB Movie Dataset
Plot Sci-Fi Action
In a post-apocalyptic world in human decay, a ... 1 0
Mohei is a wandering swordsman. He arrives in ... 0 1
#137 is a SCI/FI thriller about a girl, Marla,... 1 0
Goal: Predict movie genre based on plot summary
Categorical features generated using preprocessing


Supervised learning steps
Collect and preprocess our data
Determine a label (Example: Movie genre)
Split data into training and test sets
Extract features from the text to help predict the label
Bag-of-words vector built into scikit-learn
Evaluate trained model using the test set


________________________________________________________________________________________________


Building word count
vectors with scikitlearn

Predicting movie genre
Dataset consisting of movie plots and corresponding genre
Goal: Create bag-of-word vectors for the movie plots
Can we predict genre based on the words used in the plot
summary?


Count Vectorizer with Python
In [1]: import pandas as pd
In [2]: from sklearn.model_selection import train_test_split
In [3}: from sklearn.feature_extraction.text import CountVectorizer
In [4]: df = ... # Load data into DataFrame
In [5]: y = df['Sci-Fi']
In [6]: X_train, X_test, y_train, y_test = train_test_split(
df['plot'], y,
test_size=0.33,
random_state=53)
In [7]: count_vectorizer = CountVectorizer(stop_words='english')
In [8]: count_train = count_vectorizer.fit_transform(X_train.values)
In [9]: count_test = count_vectorizer.transform(X_test.values)


________________________________________________________________________________________________
CountVectorizer for text classification
It's time to begin building your text classifier! The data has been loaded into a DataFrame called df. Explore it in the IPython Shell to investigate what columns you can use. The .head() method is particularly informative.

In this exercise, you'll use pandas alongside scikit-learn to create a sparse text vectorizer you can use to train and test a simple supervised model. To begin, you'll set up a CountVectorizer and investigate some of its features.

Instructions
100 XP
Instructions
100 XP
Import CountVectorizer from sklearn.feature_extraction.text and train_test_split from sklearn.model_selection.
Create a Series y to use for the labels by assigning the .label attribute of df to y.
Using df["text"] (features) and y (labels), create training and test sets using train_test_split(). Use a test_size of 0.33 and a random_state of 53.
Create a CountVectorizer object called count_vectorizer. Ensure you specify the keyword argument stop_words="english" so that stop words are removed.
Fit and transform the training data X_train using the .fit_transform() method. Do the same with the test data X_test, except using the .transform() method.
Print the first 10 features of the count_vectorizer using its .get_feature_names() method.

# Import the necessary modules
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split

# Print the head of df
print(df.head())

# Create a series to store the labels: y
y = df.label

# Create training and test sets
X_train, X_test, y_train, y_test = train_test_split(df["text"],y,test_size=0.33, random_state = 53)

# Initialize a CountVectorizer object: count_vectorizer
count_vectorizer = CountVectorizer(stop_words='english')

# Transform the training data using only the 'text' column values: count_train 
count_train = count_vectorizer.fit_transform(X_train.values)

# Transform the test data using only the 'text' column values: count_test 
count_test = count_vectorizer.transform(X_test.values)

# Print the first 10 features of the count_vectorizer
print(count_vectorizer.get_feature_names()[:10])

<script.py> output:
       Unnamed: 0                                              title  \
    0        8476                       You Can Smell Hillary’s Fear   
    1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   
    2        3608        Kerry to go to Paris in gesture of sympathy   
    3       10142  Bernie supporters on Twitter erupt in anger ag...   
    4         875   The Battle of New York: Why This Primary Matters   
    
                                                    text label  
    0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  
    1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  
    2  U.S. Secretary of State John F. Kerry said Mon...  REAL  
    3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  
    4  It's primary day in New York and front-runners...  REAL  
    ['00', '000', '0000', '00000031', '000035', '00006', '0001', '0001pt', '000ft', '000km']

________________________________________________________________________________________________
TfidfVectorizer for text classification
Similar to the sparse CountVectorizer created in the previous exercise, you'll work on creating tf-idf vectors for your documents. You'll set up a TfidfVectorizer and investigate some of its features.

In this exercise, you'll use pandas and sklearn along with the same X_train, y_train and X_test, y_test DataFrames and Series you created in the last exercise.

Instructions
100 XP
Import TfidfVectorizer from sklearn.feature_extraction.text.
Create a TfidfVectorizer object called tfidf_vectorizer. When doing so, specify the keyword arguments stop_words="english" and max_df=0.7.
Fit and transform the training data.
Transform the test data.
Print the first 10 features of tfidf_vectorizer.
Print the first 5 vectors of the tfidf training data using slicing on the .A (or array) attribute of tfidf_train.

# Import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize a TfidfVectorizer object: tfidf_vectorizer
tfidf_vectorizer = TfidfVectorizer(stop_words="english",max_df=0.7)

# Transform the training data: tfidf_train 
tfidf_train = tfidf_vectorizer.fit_transform(X_train.values)

# Transform the test data: tfidf_test 
tfidf_test = tfidf_vectorizer.transform(X_test.values)

# Print the first 10 features
print(tfidf_vectorizer.get_feature_names()[:10])

# Print the first 5 vectors of the tfidf training data
print(tfidf_train.A[:5])

<script.py> output:
    ['00', '000', '001', '00684', '008s', '00am', '01', '011', '013', '016']
    [[0. 0. 0. ... 0. 0. 0.]
     [0. 0. 0. ... 0. 0. 0.]
     [0. 0. 0. ... 0. 0. 0.]
     [0. 0. 0. ... 0. 0. 0.]
     [0. 0. 0. ... 0. 0. 0.]]

________________________________________________________________________________________________

Inspecting the vectors
To get a better idea of how the vectors work, you'll investigate them by converting them into pandas DataFrames.

Here, you'll use the same data structures you created in the previous two exercises (count_train, count_vectorizer, tfidf_train, tfidf_vectorizer) as well as pandas, which is imported as pd.

Instructions
100 XP
Instructions
100 XP
Create the DataFrames count_df and tfidf_df by using pd.DataFrame() and specifying the values as the first argument and the columns (or features) as the second argument.
The values can be accessed by using the .A attribute of, respectively, count_train and tfidf_train.
The columns can be accessed using the .get_feature_names() methods of count_vectorizer and tfidf_vectorizer.
Print the head of each DataFrame to investigate their structure.
Test if the column names are the same for each DataFrame by creating a new object called difference to see the difference between the columns that count_df has from tfidf_df. Columns can be accessed using the .columns attribute of a DataFrame. Subtract the set of tfidf_df.columns from the set of count_df.columns.
Test if the two DataFrames are equivalent by using the .equals() method on count_df with tfidf_df as the argument.



# Create the CountVectorizer DataFrame: count_df
count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())

# Create the TfidfVectorizer DataFrame: tfidf_df
tfidf_df = pd.DataFrame(tfidf_train.A,columns=tfidf_vectorizer.get_feature_names())

# Print the head of count_df
print(count_df.head())

# Print the head of tfidf_df
print(tfidf_df.head())

# Calculate the difference in columns: difference
difference = set(count_df.columns) - set(tfidf_df.columns)
print(difference)

# Check whether the DataFrames are equal
print(count_df.equals(tfidf_df))



<script.py> output:
       000  01  02  02welcome  04  08  09  10  100  10th ...  œis  œour  œplease  \
    0    0   0   1          1   2   0   0   0    0     0 ...    0     0        0   
    1    4   0   0          0   0   0   0   1    0     0 ...    0     0        0   
    2    0   0   0          0   0   0   0   0    0     0 ...    0     0        0   
    3    3   1   0          0   0   0   0   0    0     1 ...    0     0        0   
    4    0   0   0          0   0   0   0   0    0     0 ...    0     0        0   
    
       œpsychosis  œpsychoticâ  œreaching  œthe  œthere  œthey  śâ  
    0           0            0          0     0       0      0   0  
    1           0            0          0     0       0      0   0  
    2           0            0          0     0       0      0   0  
    3           0            0          0     0       0      0   0  
    4           0            0          0     0       0      0   0  
    
    [5 rows x 4493 columns]
            000        01        02  02welcome        04   08   09       10  100  \
    0  0.000000  0.000000  0.103958   0.103958  0.207916  0.0  0.0  0.00000  0.0   
    1  0.034156  0.000000  0.000000   0.000000  0.000000  0.0  0.0  0.00707  0.0   
    2  0.000000  0.000000  0.000000   0.000000  0.000000  0.0  0.0  0.00000  0.0   
    3  0.079927  0.036499  0.000000   0.000000  0.000000  0.0  0.0  0.00000  0.0   
    4  0.000000  0.000000  0.000000   0.000000  0.000000  0.0  0.0  0.00000  0.0   
    
           10th ...   œis  œour  œplease  œpsychosis  œpsychoticâ  œreaching  \
    0  0.000000 ...   0.0   0.0      0.0         0.0          0.0        0.0   
    1  0.000000 ...   0.0   0.0      0.0         0.0          0.0        0.0   
    2  0.000000 ...   0.0   0.0      0.0         0.0          0.0        0.0   
    3  0.036499 ...   0.0   0.0      0.0         0.0          0.0        0.0   
    4  0.000000 ...   0.0   0.0      0.0         0.0          0.0        0.0   
    
       œthe  œthere  œthey   śâ  
    0   0.0     0.0    0.0  0.0  
    1   0.0     0.0    0.0  0.0  
    2   0.0     0.0    0.0  0.0  
    3   0.0     0.0    0.0  0.0  
    4   0.0     0.0    0.0  0.0  
    
    [5 rows x 4493 columns]
    set()
    False

________________________________________________________________________________________________

  
Training and testing a
classification model
with scikit-learn


Naive Bayes classifier
Naive Bayes Model
Commonly used for testing NLP classification problems
Basis in probability
Given a particular piece of data, how likely is a particular outcome?
Examples:
If the plot has a spaceship, how likely is it to be sci-fi?
Given a spaceship and an alien, how likely now is it sci-fi?
Each word from CountVectorizer acts as a feature
Naive Bayes: Simple and effective



Naive Bayes with scikit-learn
In [10]: from sklearn.naive_bayes import MultinomialNB
In [11]: from sklearn import metrics
In [12]: nb_classifier = MultinomialNB()
In [13]: nb_classifier.fit(count_train, y_train)
In [14]: pred = nb_classifier.predict(count_test)
In [15]: metrics.accuracy_score(y_test, pred)
Out [15]: 0.85841849389820424


Confusion Matrix
In [16]: metrics.confusion_matrix(y_test, pred, labels=[0,1])
Out [16]:
array([[6410, 563],
[ 864, 2242]])

________________________________________________________________________________________________

Training and testing the "fake news" model with CountVectorizer
Now it's your turn to train the "fake news" model using the features you identified and extracted. In this first exercise you'll train and test a Naive Bayes model using the CountVectorizer data.

The training and test sets have been created, and count_vectorizer, count_train, and count_test have been computed.

Instructions
100 XP
Instructions
100 XP
Import the metrics module from sklearn and MultinomialNB from sklearn.naive_bayes.
Instantiate a MultinomialNB classifier called nb_classifier.
Fit the classifier to the training data.
Compute the predicted tags for the test data.
Calculate and print the accuracy score of the classifier.
Compute the confusion matrix. To make it easier to read, specify the keyword argument labels=['FAKE', 'REAL'].

# Import the necessary modules
from sklearn import metrics
from sklearn.naive_bayes import MultinomialNB

# Instantiate a Multinomial Naive Bayes classifier: nb_classifier
nb_classifier = MultinomialNB()

# Fit the classifier to the training data
nb_classifier.fit(count_train, y_train)

# Create the predicted tags: pred
pred = nb_classifier.predict(count_test)

# Calculate the accuracy score: score
score = metrics.accuracy_score(y_test, pred)
print(score)

# Calculate the confusion matrix: cm
cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE','REAL'])
print(cm)


<script.py> output:
    0.893352462936394
    [[ 865  143]
     [  80 1003]]
________________________________________________________________________________________________

Training and testing the "fake news" model with TfidfVectorizer
Now that you have evaluated the model using the CountVectorizer, you'll do the same using the TfidfVectorizer with a Naive Bayes model.

The training and test sets have been created, and tfidf_vectorizer, tfidf_train, and tfidf_test have been computed. Additionally, MultinomialNB and metrics have been imported from, respectively, sklearn.naive_bayes and sklearn.

Instructions
100 XP
Instantiate a MultinomialNB classifier called nb_classifier.
Fit the classifier to the training data.
Compute the predicted tags for the test data.
Calculate and print the accuracy score of the classifier.
Compute the confusion matrix. As in the previous exercise, specify the keyword argument labels=['FAKE', 'REAL'] so that the resulting confusion matrix is easier to read.

# Create a Multinomial Naive Bayes classifier: nb_classifier
nb_classifier = MultinomialNB()

# Fit the classifier to the training data
nb_classifier.fit(tfidf_train, y_train)

# Create the predicted tags: pred
pred = nb_classifier.predict(tfidf_test)

# Calculate the accuracy score: score
score = metrics.accuracy_score(y_test, pred)
print(score)

# Calculate the confusion matrix: cm
cm = metrics.confusion_matrix(y_test, pred, labels=['FAKE','REAL'])
print(cm)


<script.py> output:
    0.8565279770444764
    [[ 739  269]
     [  31 1052]]
________________________________________________________________________________________________


Simple NLP, Complex
Problems

Improving your model
Your job in this exercise is to test a few different alpha levels using the Tfidf vectors to determine if there is a better performing combination.

The training and test sets have been created, and tfidf_vectorizer, tfidf_train, and tfidf_test have been computed.

Instructions
100 XP
Create a list of alphas to try using np.arange(). Values should range from 0 to 1 with steps of 0.1.
Create a function train_and_predict() that takes in one argument: alpha. The function should:
Instantiate a MultinomialNB classifier with alpha=alpha.
Fit it to the training data.
Compute predictions on the test data.
Compute and return the accuracy score.
Using a for loop, print the alpha, score and a newline in between. Use your train_and_predict() function to compute the score. Does the score change along with the alpha? What is the best alpha?

# Create the list of alphas: alphas
alphas = np.arange(0,1,0.1)

# Define train_and_predict()
def train_and_predict(alpha):
    # Instantiate the classifier: nb_classifier
    nb_classifier = MultinomialNB(alpha=alpha)
    # Fit to the training data
    nb_classifier.fit(tfidf_train, y_train)
    # Predict the labels: pred
    pred = nb_classifier.predict(tfidf_test)
    # Compute accuracy: score
    score = metrics.accuracy_score(y_test, pred)
    return score

# Iterate over the alphas and print the corresponding score
for alpha in alphas:
    print('Alpha: ', alpha)
    print('Score: ', train_and_predict(alpha))
    print()

<script.py> output:
    Alpha:  0.0
    Score:  0.8813964610234337
    
    Alpha:  0.1
    Score:  0.8976566236250598
    
    Alpha:  0.2
    Score:  0.8938307030129125
    
    Alpha:  0.30000000000000004
    Score:  0.8900047824007652
    
    Alpha:  0.4
    Score:  0.8857006217120995
    
    Alpha:  0.5
    Score:  0.8842659014825442
    
    Alpha:  0.6000000000000001
    Score:  0.874701099952176
    
    Alpha:  0.7000000000000001
    Score:  0.8703969392635102
    
    Alpha:  0.8
    Score:  0.8660927785748446
    
    Alpha:  0.9
    Score:  0.8589191774270684



________________________________________________________________________________________________


Inspecting your model
Now that you have built a "fake news" classifier, you'll investigate what it has learned. You can map the important vector weights back to actual words using some simple inspection techniques.

You have your well performing tfidf Naive Bayes classifier available as nb_classifier, and the vectors as tfidf_vectorizer.

Instructions
100 XP
Save the class labels as class_labels by accessing the .classes_ attribute of nb_classifier.
Extract the features using the .get_feature_names() method of tfidf_vectorizer.
Create a zipped array of the classifier coefficients with the feature names and sort them by the coefficients. To do this, first use zip() with the arguments nb_classifier.coef_[0] and feature_names. Then, use sorted() on this.
Print the top 20 weighted features for the first label of class_labels.
Print the bottom 20 weighted features for the second label of class_labels.

# Get the class labels: class_labels
class_labels = nb_classifier.classes_

# Extract the features: feature_names
feature_names = tfidf_vectorizer.get_feature_names()

# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights
feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))

# Print the first class label and the top 20 feat_with_weights entries
print(class_labels[0], feat_with_weights[:20])

# Print the second class label and the bottom 20 feat_with_weights entries
print(class_labels[1], feat_with_weights[-20:])



<script.py> output:
    FAKE [(-12.641778440826338, '0000'), (-12.641778440826338, '000035'), (-12.641778440826338, '0001'), (-12.641778440826338, '0001pt'), (-12.641778440826338, '000km'), (-12.641778440826338, '0011'), (-12.641778440826338, '006s'), (-12.641778440826338, '007'), (-12.641778440826338, '007s'), (-12.641778440826338, '008s'), (-12.641778440826338, '0099'), (-12.641778440826338, '00am'), (-12.641778440826338, '00p'), (-12.641778440826338, '00pm'), (-12.641778440826338, '014'), (-12.641778440826338, '015'), (-12.641778440826338, '018'), (-12.641778440826338, '01am'), (-12.641778440826338, '020'), (-12.641778440826338, '023')]
    REAL [(-6.790929954967984, 'states'), (-6.765360557845786, 'rubio'), (-6.751044290367751, 'voters'), (-6.701050756752027, 'house'), (-6.695547793099875, 'republicans'), (-6.6701912490429685, 'bush'), (-6.661945235816139, 'percent'), (-6.589623788689862, 'people'), (-6.559670340096453, 'new'), (-6.489892292073901, 'party'), (-6.452319082422527, 'cruz'), (-6.452076515575875, 'state'), (-6.397696648238072, 'republican'), (-6.376343060363355, 'campaign'), (-6.324397735392007, 'president'), (-6.2546017970213645, 'sanders'), (-6.144621899738043, 'obama'), (-5.756817248152807, 'clinton'), (-5.596085785733112, 'said'), (-5.357523914504495, 'trump')]

